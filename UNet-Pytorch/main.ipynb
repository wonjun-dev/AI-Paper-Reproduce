{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_dir = 'data/images'\n",
    "target_dir = 'data/annotations/trimaps'\n",
    "img_size = (160, 160)\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "\n",
    "input_img_paths = sorted([os.path.join(input_dir, fname) \n",
    "                            for fname in os.listdir(input_dir)\n",
    "                            if fname.endswith('.jpg')])\n",
    "target_img_paths = sorted([os.path.join(target_dir, fname) \n",
    "                            for fname in os.listdir(target_dir)\n",
    "                            if fname.endswith('.png') and not fname.startswith('.')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "display(Image.open(input_img_paths[7]))\n",
    "display(PIL.ImageOps.autocontrast(Image.open(target_img_paths[7])))\n",
    "# display(Image.open(target_img_paths[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, img_size, input_img_paths, target_img_paths, transform=None, target_transform=None):\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(input_img_paths[index]).convert(\"RGB\")\n",
    "        target = np.array(Image.open(target_img_paths[index]))\n",
    "        target = target - np.ones_like(target)\n",
    "        target = torch.from_numpy(target).unsqueeze(0).to(dtype=torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return image, target\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((160, 160))])\n",
    "target_transform = transforms.Compose([transforms.Resize((160, 160))])\n",
    "\n",
    "dataset = OxfordPetDataset(img_size=(160, 160), input_img_paths=input_img_paths, target_img_paths=target_img_paths, transform=transform, target_transform=target_transform)\n",
    "print(dataset.__len__())\n",
    "image, target = next(iter(dataset))\n",
    "print(image ,target)\n",
    "print(torch.unique(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "class OxfordPetDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, img_size, batch_size, input_img_paths, target_img_paths):\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((160, 160))])\n",
    "        target_transform = transforms.Compose([transforms.Resize((160, 160))])\n",
    "        dataset = OxfordPetDataset(img_size=self.img_size, input_img_paths=self.input_img_paths, target_img_paths=self.target_img_paths, transform=transform, target_transform=target_transform)\n",
    "        self.train_dataset, self.valid_dataset = random_split(dataset, [7000, 390])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        oxford_train = DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=8, shuffle=True)\n",
    "        return oxford_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        oxford_valid = DataLoader(self.valid_dataset, batch_size=self.batch_size, num_workers=8, shuffle=False)\n",
    "        return oxford_valid\n",
    "\n",
    "img_size = (160, 160)\n",
    "batch_size = 64\n",
    "dm = OxfordPetDataModule(img_size, batch_size, input_img_paths, target_img_paths)\n",
    "dm.setup()\n",
    "dataloader = dm.train_dataloader()\n",
    "# for x, y in dataloader:\n",
    "#     print(x.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    out = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def conv_block_2x(in_channels, out_channels):\n",
    "    out = nn.Sequential(\n",
    "        conv_block(in_channels, out_channels),\n",
    "        conv_block(out_channels, out_channels)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def downsample():\n",
    "    return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "def upsample(in_channels, out_channels):\n",
    "    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "\n",
    "class UNet(pl.LightningModule):\n",
    "    def __init__(self, input_dim, num_classes, num_channels):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # conv_block_2x\n",
    "        self.encoder_1 = conv_block_2x(input_dim, num_channels)\n",
    "        # downsample\n",
    "        self.down_sample_1 = downsample()\n",
    "        # conv_block_2x\n",
    "        self.encoder_2 = conv_block_2x(num_channels, num_channels * 2)\n",
    "        # downsample\n",
    "        self.down_sample_2 = downsample()\n",
    "        # conv_block_2x\n",
    "        self.encoder_3 = conv_block_2x(num_channels * 2, num_channels * 4)\n",
    "        # downsample\n",
    "        self.down_sample_3 = downsample()\n",
    "        # conv_block_2x\n",
    "        self.encoder_4 = conv_block_2x(num_channels * 4, num_channels * 8)\n",
    "        # downsample\n",
    "        self.down_sample_4 = downsample()\n",
    "        # bridge\n",
    "        self.bridge = conv_block_2x(num_channels * 8, num_channels * 16)\n",
    "        # upsample\n",
    "        self.up_sample_1 = upsample(num_channels * 16, num_channels * 8)\n",
    "        # conv_block_2x\n",
    "        self.decoder_1 = conv_block_2x(num_channels * 16, num_channels * 8)\n",
    "        # upsample\n",
    "        self.up_sample_2 = upsample(num_channels * 8, num_channels * 4)\n",
    "        # conv_block_2x\n",
    "        self.decoder_2 = conv_block_2x(num_channels * 8, num_channels * 4)\n",
    "        # upsample\n",
    "        self.up_sample_3 = upsample(num_channels * 4, num_channels * 2)\n",
    "        # conv_block_2x\n",
    "        self.decoder_3 = conv_block_2x(num_channels * 4, num_channels * 2)\n",
    "        # upsample\n",
    "        self.up_sample_4 = upsample(num_channels * 2, num_channels)\n",
    "        # conv_block_2x\n",
    "        self.decoder_4 = conv_block_2x(num_channels * 2, num_channels)\n",
    "        # 1X1 conv\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_classes, kernel_size=1, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        encoder_feat_1 = self.encoder_1(input)\n",
    "        down_feat_1 = self.down_sample_1(encoder_feat_1)\n",
    "        encoder_feat_2 = self.encoder_2(down_feat_1)\n",
    "        down_feat_2 = self.down_sample_2(encoder_feat_2)\n",
    "        encoder_feat_3 = self.encoder_3(down_feat_2)\n",
    "        down_feat_3 = self.down_sample_3(encoder_feat_3)\n",
    "        encoder_feat_4 = self.encoder_4(down_feat_3)\n",
    "        down_feat_4 = self.down_sample_4(encoder_feat_4)\n",
    "\n",
    "        bridge_feat = self.bridge(down_feat_4)\n",
    "\n",
    "        up_feat_1 = self.up_sample_1(bridge_feat)\n",
    "        decoder_feat_1 = self.decoder_1(torch.cat((encoder_feat_4, up_feat_1), dim=1))\n",
    "        up_feat_2 = self.up_sample_2(decoder_feat_1)\n",
    "        decoder_feat_2 = self.decoder_2(torch.cat((encoder_feat_3, up_feat_2), dim=1))\n",
    "        up_feat_3 = self.up_sample_3(decoder_feat_2)\n",
    "        decoder_feat_3 = self.decoder_3(torch.cat((encoder_feat_2, up_feat_3), dim=1))\n",
    "        up_feat_4 = self.up_sample_4(decoder_feat_3)\n",
    "        decoder_feat_4 = self.decoder_4(torch.cat((encoder_feat_1, up_feat_4), dim=1))\n",
    "\n",
    "        out = self.classifier(decoder_feat_4)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = F.cross_entropy(pred, y.squeeze())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = F.cross_entropy(pred, y.squeeze())\n",
    "        self.log('val_loss', loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        return optimizer\n",
    "        \n",
    "\n",
    "# unet = UNet(input_dim=3, num_classes=3, num_channels=64)\n",
    "# for x, y in dataloader:\n",
    "#     print(x.shape, y.shape)\n",
    "#     print(unet(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name          | Type            | Params\n",
      "---------------------------------------------------\n",
      "0  | encoder_1     | Sequential      | 39.0 K\n",
      "1  | down_sample_1 | MaxPool2d       | 0     \n",
      "2  | encoder_2     | Sequential      | 221 K \n",
      "3  | down_sample_2 | MaxPool2d       | 0     \n",
      "4  | encoder_3     | Sequential      | 886 K \n",
      "5  | down_sample_3 | MaxPool2d       | 0     \n",
      "6  | encoder_4     | Sequential      | 3.5 M \n",
      "7  | down_sample_4 | MaxPool2d       | 0     \n",
      "8  | bridge        | Sequential      | 14.2 M\n",
      "9  | up_sample_1   | ConvTranspose2d | 2.1 M \n",
      "10 | decoder_1     | Sequential      | 7.1 M \n",
      "11 | up_sample_2   | ConvTranspose2d | 524 K \n",
      "12 | decoder_2     | Sequential      | 1.8 M \n",
      "13 | up_sample_3   | ConvTranspose2d | 131 K \n",
      "14 | decoder_3     | Sequential      | 443 K \n",
      "15 | up_sample_4   | ConvTranspose2d | 32.8 K\n",
      "16 | decoder_4     | Sequential      | 110 K \n",
      "17 | classifier    | Sequential      | 195   \n",
      "---------------------------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "124.175   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 117/117 [01:31<00:00,  1.28it/s, loss=0.387, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "from gc import callbacks\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "model = UNet(input_dim=3, num_classes=3, num_channels=64)\n",
    "data_module = OxfordPetDataModule(img_size, batch_size, input_img_paths, target_img_paths)\n",
    "ckpt_callback = ModelCheckpoint(save_top_k=2, monitor='val_loss', mode='min')\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10, callbacks=[ckpt_callback])\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = './test'\n",
    "\n",
    "test_img_paths = sorted([os.path.join(test_dir, fname)\n",
    "                            for fname in os.listdir(test_dir)\n",
    "                            if fname.endswith('.jpg')])\n",
    "print(test_img_paths)\n",
    "\n",
    "model = UNet.load_from_checkpoint('./lightning_logs/version_1/checkpoints/epoch=9-step=1099.ckpt')\n",
    "\n",
    "for img in test_img_paths:                            \n",
    "    image = Image.open(img).convert(\"RGB\")\n",
    "    display(image)\n",
    "    image_size = image.size\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((160, 160))])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize((image_size[1], image_size[0]))])\n",
    "    pred_mask = transform(torch.argmax(model(image), dim=1).to(dtype=torch.float))\n",
    "    display(PIL.ImageOps.autocontrast(pred_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3ad68e2cf6b7a9cd66ec2cf265114c7e0ee6b63e37055247c1292e5c87552b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
