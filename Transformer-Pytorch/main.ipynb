{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wonjun/Desktop/projects/AI-Paper-Reproduce/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Set tokenizer and build vocabulary\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_md')\n",
    "\n",
    "def yield_tokens(data_iter, language: str):\n",
    "    for text in data_iter:\n",
    "        if language == 'en':\n",
    "            yield en_tokenizer(text[0])\n",
    "        elif language == 'de':\n",
    "            yield de_tokenizer(text[1])\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'en'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "de_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'de'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "en_vocab.set_default_index(UNK_IDX) # oov 일때 반환하는 토큰\n",
    "de_vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165, 36, 7, 335, 287, 17, 1224, 4, 758, 4496, 2957, 5]\n",
      "[84, 31, 10, 847, 2208, 15, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "# Set preprocess pipeline\n",
    "en_pipeline = lambda x: en_vocab(en_tokenizer(x))\n",
    "de_pipeline = lambda x: de_vocab(de_tokenizer(x))\n",
    "print(en_pipeline('Several men in hard hats are operating a giant pulley system.'))\n",
    "print(de_pipeline('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
    "train_loader = DataLoader(train_iter, batch_size=2)\n",
    "for src, data in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer\n",
    "# I follow style of official Pytorch Transformer source code and adjust it simply\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.modules import LayerNorm\n",
    "from torch.nn.parameter import Parameter\n",
    "import pytorch_lightning as pl\n",
    "from typing import Union, Callable, Optional, Any, Tuple\n",
    "\n",
    "\n",
    "class TransformerWrapper(pl.LightningModule):\n",
    "    def __init__(self, transformer):\n",
    "        self.model = transformer\n",
    "    def training_step(self, *args: Any, **kwargs: Any):\n",
    "        return super().training_step(*args, **kwargs)\n",
    "    def validation_step(self, *args: Any, **kwargs: Any):\n",
    "        return super().validation_step(*args, **kwargs)\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return super().configure_optimizers()\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_head: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, \\\n",
    "        dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \\\n",
    "        layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Transformer, self).__init__()\n",
    "        # TODO \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, n_head, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)\n",
    "        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = None\n",
    "        decoder_norm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        self._reset_params()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, \\\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "                memory = self.encoder()\n",
    "                output  = self.decoder()\n",
    "                return output\n",
    "    \n",
    "    def _reset_params(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dim_feedforward: int = 2048, dropout: float = 0.1, \\\n",
    "                activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,  \n",
    "                device=None, dtype=None) -> None:\n",
    "\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Define params\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, dropout, batch_first, **factory_kwargs)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.droput = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._self_attn_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            x = x + self._feedforward_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._self_attn_block(x, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._feedforward_block(x))\n",
    "        return x\n",
    "\n",
    "    def _self_attn_block(self, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    def _feedforward_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout2(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0, batch_first=False, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = int(embed_dim // num_heads)\n",
    "        assert self.embed_dim == self.head_dim * num_heads\n",
    "\n",
    "        # self.in_proj_weight = Parameter(torch.empty((3*embed_dim, embed_dim), **factory_kwargs))\n",
    "        # self.register_parameter('q_proj_weight', None)\n",
    "        # self.register_parameter('k_proj_weight', None)\n",
    "        # self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, need_weights: bool = True, \\\n",
    "                attn_mask: Optional[Tensor] = None, average_attn_weights: bool = True) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        query, key, value = self.wq(query), self.wk(key), self.wv(value)\n",
    "        query, key ,value = self._split_heads(query), self._split_heads(key), self._split_heads(value)\n",
    "        attn_out = self._attention(query, key, value)\n",
    "        attn_out = self.out_proj(attn_out)\n",
    "        return attn_out\n",
    "    \n",
    "    def _split_heads(self, proj: Tensor) -> Tensor:\n",
    "        if self.batch_first:    # (N, L, E)\n",
    "            bs = proj.size(0)\n",
    "            proj = proj.view(bs, -1, self.num_heads, self.head_dim)    # (N, L, H, E_Hi)\n",
    "            proj = proj.transpose(1, 2) # (N, H, L, E_Hi)\n",
    "        else:   # (L, N, E)\n",
    "            bs = proj.size(1)\n",
    "            proj = proj.view(-1, bs, self.num_heads, self.head_dim)    # (L, N, H, E_Hi)\n",
    "        return proj\n",
    "    \n",
    "    def _attention(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        if self.batch_first:\n",
    "            score = torch.matmul(query, key.transpose(-1, -2))  # (N, H, L, L), score = Q * K^T\n",
    "            score = score / math.sqrt(query.size(-1)) # score = Q * K^T divided by sqrt(E_Hi)\n",
    "            softmax = F.softmax(score, dim=-1)\n",
    "            attn_out = torch.matmul(score, value)   # (N, H, L, E_Hi)\n",
    "            attn_out = attn_out.transpose(1, 2) # (N, L, H, E_Hi)\n",
    "            attn_out = attn_out.contiguous().view(attn_out.size(0), -1, self.embed_dim)  # (N, L, E)\n",
    "            return attn_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (droput): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (droput): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (droput): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (droput): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (droput): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (droput): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_src = torch.rand((5, 10, 512))\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "encoder_layer = TransformerEncoderLayer(d_model, n_head, batch_first=True)\n",
    "encoder = TransformerEncoder(encoder_layer, num_layers=6)\n",
    "out = encoder(dummy_src)\n",
    "encoder.parameters\n",
    "# commit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d54d44b476ee03c59f2b2873b2d13a0a58dbc0ce917a1197d2792a3bd70a14c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
