{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer and build vocabulary\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_md')\n",
    "\n",
    "def yield_tokens(data_iter, language: str):\n",
    "    for text in data_iter:\n",
    "        if language == 'en':\n",
    "            yield en_tokenizer(text[0])\n",
    "        elif language == 'de':\n",
    "            yield de_tokenizer(text[1])\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'en'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "de_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'de'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "en_vocab.set_default_index(UNK_IDX) # oov 일때 반환하는 토큰\n",
    "de_vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166, 37, 8, 336, 288, 18, 1225, 4, 759, 4497, 2958, 6]\n",
      "[85, 32, 11, 848, 2209, 16, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "# Set preprocess pipeline\n",
    "en_pipeline = lambda x: en_vocab(en_tokenizer(x))\n",
    "de_pipeline = lambda x: de_vocab(de_tokenizer(x))\n",
    "trasnform_pipeline = {'en': en_pipeline, 'de': de_pipeline}\n",
    "print(en_pipeline('Several men in hard hats are operating a giant pulley system.'))\n",
    "print(de_pipeline('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_func(batch, src_ln: str = 'en', tgt_ln: str = 'de', batch_first: bool = True):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src, tgt in batch:\n",
    "        src_ids = trasnform_pipeline[src_ln](src.rstrip('\\n'))\n",
    "        tgt_ids = trasnform_pipeline[tgt_ln](tgt.rstrip('\\n'))\n",
    "        src_ids = torch.cat((torch.tensor([BOS_IDX]), torch.tensor(src_ids), torch.tensor([EOS_IDX])))\n",
    "        tgt_ids = torch.cat((torch.tensor([BOS_IDX]), torch.tensor(tgt_ids), torch.tensor([EOS_IDX])))\n",
    "        src_batch.append(src_ids)\n",
    "        tgt_batch.append(tgt_ids)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=batch_first)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=batch_first)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), language_pair=('en', 'de'))\n",
    "train_loader = DataLoader(train_iter, batch_size=2, collate_fn=collate_func)\n",
    "valid_loader = DataLoader(valid_iter, batch_size=1, collate_fn=collate_func)\n",
    "test_loader = DataLoader(test_iter, batch_size=1, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer\n",
    "# I follow style of official Pytorch Transformer source code and adjust it simply\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.modules import LayerNorm\n",
    "from typing import Union, Callable, Optional, Any, Tuple\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_head: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, \\\n",
    "        dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \\\n",
    "        layer_norm_eps: float = 1e-5, batch_first: bool = True, norm_first: bool = False, device=None, dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Transformer, self).__init__()\n",
    "   \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, n_head, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)\n",
    "        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_head, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)\n",
    "        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_params()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, \\\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output  = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "    \n",
    "    def _reset_params(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dim_feedforward: int = 2048, dropout: float = 0.1, \\\n",
    "                activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,  \n",
    "                device=None, dtype=None) -> None:\n",
    "\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Define params\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, dropout, batch_first, **factory_kwargs)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.droput = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._self_attn_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            x = x + self._feedforward_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._self_attn_block(x, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._feedforward_block(x))\n",
    "        return x\n",
    "\n",
    "    def _self_attn_block(self, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    def _feedforward_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.droput(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dim_feedforward: int = 2048, droput: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \\\n",
    "                layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        # Define params\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, droput, batch_first, **factory_kwargs)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, n_head, droput, batch_first, **factory_kwargs)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.droput = nn.Dropout(droput)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(droput)\n",
    "        self.dropout2 = nn.Dropout(droput)\n",
    "        self.dropout3 = nn.Dropout(droput)\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._self_attn_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._multihead_attn_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
    "            x = x + self._feedforward_block(self.norm3(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._self_attn_block(x, tgt_mask ,tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._multihead_attn_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "            x = self.norm3(x + self._feedforward_block(x))\n",
    "        return x\n",
    "    \n",
    "    def _self_attn_block(self, x: Tensor, attn_mask: Optional[Tensor] = None, key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _multihead_attn_block(self, x: Tensor, mem: Tensor, attn_mask: Optional[Tensor] = None, key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = self.multihead_attn(x, mem, mem, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout2(x)\n",
    "    \n",
    "    def _feedforward_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.droput(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0, batch_first=False, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = int(embed_dim // num_heads)\n",
    "        assert self.embed_dim == self.head_dim * num_heads\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, need_weights: bool = True, \\\n",
    "                attn_mask: Optional[Tensor] = None, average_attn_weights: bool = True) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        query, key, value = self.wq(query), self.wk(key), self.wv(value)\n",
    "        query, key ,value = self._split_heads(query), self._split_heads(key), self._split_heads(value)\n",
    "        attn_out = self._attention(query, key, value, key_padding_mask, attn_mask)\n",
    "        attn_out = self.out_proj(attn_out)\n",
    "        return attn_out\n",
    "    \n",
    "    def _split_heads(self, proj: Tensor) -> Tensor:\n",
    "        if self.batch_first:    # (N, L, E)\n",
    "            bs = proj.size(0)\n",
    "            proj = proj.view(bs, -1, self.num_heads, self.head_dim)    # (N, L, H, E_Hi)\n",
    "            proj = proj.transpose(1, 2) # (N, H, L, E_Hi)\n",
    "        else:   # (L, N, E)\n",
    "            bs = proj.size(1)\n",
    "            proj = proj.view(-1, bs, self.num_heads, self.head_dim)    # (L, N, H, E_Hi)\n",
    "        return proj\n",
    "    \n",
    "    def _attention(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, attn_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        if self.batch_first:\n",
    "            score = torch.matmul(query, key.transpose(-1, -2))  # (N, H, L, L), score = Q * K^T\n",
    "            score = score / math.sqrt(query.size(-1)) # score = Q * K^T divided by sqrt(E_Hi)\n",
    "            print('score_shape', score.shape)\n",
    "            if key_padding_mask is not None:\n",
    "                score = score.masked_fill(key_padding_mask == 0, float('-inf'))\n",
    "                print(score)\n",
    "            if attn_mask is not None:\n",
    "                score = score.masked_fill(attn_mask == 0, float('-inf'))\n",
    "                print(score)\n",
    "\n",
    "            softmax = F.softmax(score, dim=-1)\n",
    "            attn_out = torch.matmul(softmax, value)   # (N, H, L, E_Hi)\n",
    "            attn_out = attn_out.transpose(1, 2) # (N, L, H, E_Hi)\n",
    "            attn_out = attn_out.contiguous().view(attn_out.size(0), -1, self.embed_dim)  # (N, L, E)\n",
    "            return attn_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int = 512, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1)\n",
    "        val = torch.exp(-torch.arange(0, emb_size, 2) / emb_size * math.log(10000))\n",
    "        pos_encoding = torch.zeros((max_len, emb_size))\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos * val)\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos * val)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)    # batch first\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "    \n",
    "    def forward(self, token_embedding: Tensor) -> Tensor:\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:, :token_embedding.size(1), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int = 512) -> None:\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def forward(self, tokens: Tensor) -> Tensor:\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWrapper(nn.Module):\n",
    "    def __init__(self, emb_size: int, src_vocab_size: int, tgt_vocab_size: int, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size, batch_first=True)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, src_padding_mask: Optional[Tensor] = None, tgt_padding_mask: Optional[Tensor] = None, memory_padding_mask: Optional[Tensor] = None):\n",
    "        src_emb = self.pos_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.pos_encoding(self.tgt_tok_emb(tgt))\n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask, src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_padding_mask)\n",
    "        return self.generator(output)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        src_emb = self.pos_encoding(self.src_tok_emb(src))\n",
    "        return self.transformer.encoder(src_emb, src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor) -> Tensor:\n",
    "        tgt_emb = self.pos_encoding(self.tgt_tok_emb(tgt))\n",
    "        return self.transformer.decoder(tgt_emb, memory, tgt_mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "\n",
    "class TransformerTrainer(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.transformer = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, tgt = batch    # (N, L)\n",
    "        logits = self.transformer(src, tgt[:, :-1])    # (N, L, tgt_vacab_size)\n",
    "        loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        self.log('train loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src: Tensor, tgt: Tensor) -> Tuple[Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src: [N, L]\n",
    "        tgt: [N, L]\n",
    "    \"\"\"\n",
    "    src_seq_len = src.shape[-1] # L\n",
    "    tgt_seq_len = tgt.shape[-1]\n",
    "\n",
    "    \"\"\"\n",
    "    tgt mask\n",
    "    1 0 0 0 0 \n",
    "    1 1 0 0 0\n",
    "    1 1 1 0 0\n",
    "    1 1 1 1 0\n",
    "    1 1 1 1 1\n",
    "    \"\"\"\n",
    "    tgt_mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1).float()\n",
    "    src_mask = torch.ones((src_seq_len, src_seq_len)).type(torch.bool)\n",
    "    src_padding_mask = (src != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "    tgt_padding_mask = (tgt != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "           0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "           1., 1., 1.]]]])\n",
      "torch.Size([19, 19])\n",
      "torch.Size([2, 1, 1, 20])\n",
      "torch.Size([2, 1, 1, 19])\n",
      "score_shape torch.Size([2, 8, 20, 20])\n",
      "tensor([[[[ 3.4218e+02, -8.6976e+02,  3.6668e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.2197e+03,  9.5816e+01,  3.3452e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-3.9936e+02,  3.9866e+02,  1.0080e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-8.2364e+02, -2.2045e+02,  5.7169e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.8334e+02, -4.1499e+02,  2.2163e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-4.4546e+02, -1.4030e+02,  6.2033e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 1.6142e+02,  2.1670e+02,  2.9084e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.0911e+02, -1.3317e+02, -4.5254e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.7861e+02, -4.5345e+02,  1.9414e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 8.9539e+02,  1.4757e+02, -3.0913e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 9.9351e+02,  6.3356e+01, -1.2240e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.0284e+03,  9.0634e+01, -3.3612e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-2.9516e+02,  5.9295e+02,  4.0545e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.7080e+02,  1.0050e+02,  1.0152e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.9117e+02, -3.0927e+02, -2.1242e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-4.4971e+02, -1.7594e+02,  2.6345e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-6.7543e+02, -1.8746e+02,  4.6411e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-4.3747e+02, -1.0438e+02,  4.5446e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1061e+02,  2.9906e+02,  9.4297e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.1285e+02, -1.1443e+02, -2.1469e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.3643e+02,  1.5105e+03, -6.1317e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 8.3992e+01,  4.5260e+02,  1.1021e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 3.2910e+02,  4.6341e+02,  4.6066e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.4124e+02,  4.7980e+02, -2.3059e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 9.8267e+01,  1.0094e+03, -1.0955e+03,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 7.1445e+01,  4.2374e+02,  5.2515e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 4.3379e+02, -6.0948e+02, -5.5811e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-5.0384e+02,  8.3337e+02, -5.0413e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-7.3357e+02,  7.1496e+02, -7.6455e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-6.0850e+02,  4.0839e+02, -8.3264e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-2.9636e+02, -1.9687e+02,  6.1549e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-7.5045e+02,  1.0902e+03, -2.1297e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-9.4091e+02,  3.7661e+02, -8.4052e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 2.2157e+02, -7.8715e+02,  1.2538e+03,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.9209e+02, -4.7450e+02,  1.3106e+03,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.4853e+02, -5.5079e+02,  1.4661e+03,  ...,        -inf,\n",
      "                  -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1395e+02, -9.2655e+02, -5.7565e+01,  ...,  2.9651e+02,\n",
      "           -7.3549e+02, -7.2898e+02],\n",
      "          [-1.0786e+03, -3.2513e+01, -1.0779e+03,  ..., -5.8853e+02,\n",
      "           -1.8400e+02,  3.6196e+02],\n",
      "          [ 1.0304e+03, -8.7322e+02, -3.8980e+02,  ..., -4.2369e+02,\n",
      "            4.1440e+02, -2.4231e+02],\n",
      "          ...,\n",
      "          [-2.2308e+02, -1.4340e+03, -4.2642e+02,  ..., -1.3072e+03,\n",
      "            6.3081e+02, -7.1862e+01],\n",
      "          [ 8.6040e+02, -1.5548e+02,  2.8807e+02,  ..., -1.6388e+02,\n",
      "            1.0623e+03, -5.8511e+02],\n",
      "          [-5.9149e+02, -2.9463e+02, -6.0810e+02,  ..., -3.3882e+02,\n",
      "           -5.2203e+02, -5.4020e+00]],\n",
      "\n",
      "         [[-4.2043e+02,  4.5077e+02,  1.1635e+03,  ...,  6.0400e+02,\n",
      "           -2.5798e+02,  3.8241e+01],\n",
      "          [-2.4259e+02, -1.3059e+02, -2.3037e+02,  ..., -6.4703e+02,\n",
      "            2.4123e+02,  7.0558e+02],\n",
      "          [-8.0497e+02, -8.3345e+02, -3.3298e+02,  ...,  4.6661e+02,\n",
      "           -1.2122e+02, -9.2582e+02],\n",
      "          ...,\n",
      "          [-5.6490e+01, -1.7207e+02, -1.1182e+03,  ..., -6.1483e+02,\n",
      "            6.9552e+01,  2.8832e+02],\n",
      "          [-2.5920e+02,  1.8681e+02, -1.8339e+02,  ...,  6.1840e+02,\n",
      "            2.0889e+00, -6.8624e+02],\n",
      "          [ 4.1073e+02,  5.1179e+02,  2.8795e+01,  ...,  2.3313e+02,\n",
      "            9.9293e+02,  5.2721e+02]],\n",
      "\n",
      "         [[-5.8166e+01,  9.4666e+02,  7.3621e+01,  ...,  1.0152e+02,\n",
      "           -9.2178e+02,  1.3250e+02],\n",
      "          [-3.4675e+02,  1.2949e+00,  5.8701e+01,  ...,  8.2133e+02,\n",
      "           -1.0451e+03,  5.9005e+02],\n",
      "          [-5.0862e+02,  2.7825e+02,  1.3851e+02,  ..., -1.3049e+03,\n",
      "           -2.7396e+02,  1.4996e+02],\n",
      "          ...,\n",
      "          [-1.1856e+02, -7.7038e+02, -2.0630e+02,  ...,  4.4276e+02,\n",
      "            3.7406e+02,  6.5805e+02],\n",
      "          [-6.1899e+02,  3.5322e+02,  1.9219e+02,  ..., -1.6310e+03,\n",
      "            1.8652e+03, -2.3878e+02],\n",
      "          [ 5.9423e+02, -7.0770e+02, -1.2088e+02,  ...,  1.5620e+03,\n",
      "            1.0650e+03,  8.3290e+02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.3452e+02, -1.5101e+02,  3.2515e+02,  ...,  6.6688e+01,\n",
      "            4.5363e+01,  1.1061e+02],\n",
      "          [-5.3735e+02,  6.2385e+02,  6.2291e+02,  ...,  2.4231e+02,\n",
      "            8.6995e+02, -6.6978e+02],\n",
      "          [ 5.0686e+01,  3.4170e+02, -6.8151e+02,  ..., -7.8994e+02,\n",
      "            3.8432e+02,  2.0204e+01],\n",
      "          ...,\n",
      "          [-3.9039e+02, -7.2712e+02,  3.1581e+02,  ..., -5.2795e+02,\n",
      "            5.8145e+02, -4.9554e+02],\n",
      "          [ 8.0961e+02, -3.8641e+02,  1.3288e+02,  ..., -7.4137e+02,\n",
      "           -1.3921e+02,  1.0953e+02],\n",
      "          [ 2.8697e+01,  1.0901e+03, -1.9392e+02,  ...,  1.7334e+02,\n",
      "            2.1969e+00, -2.6716e+02]],\n",
      "\n",
      "         [[ 4.0026e+02,  6.4654e+02, -1.6552e+02,  ...,  4.4782e+02,\n",
      "            1.5644e+02, -4.3960e+02],\n",
      "          [-1.0859e+02,  8.4824e+02,  6.1296e+02,  ...,  5.8922e+02,\n",
      "            3.6573e+02, -2.5665e+02],\n",
      "          [ 6.6824e+02,  6.0980e+01, -1.2653e+02,  ..., -3.1376e+02,\n",
      "           -2.0896e+02,  1.1052e+01],\n",
      "          ...,\n",
      "          [-3.2839e+02, -2.4641e+02, -5.9611e+01,  ...,  3.7329e+02,\n",
      "            4.1883e+02, -6.2804e+02],\n",
      "          [-6.5011e+01, -1.3560e+03, -1.0535e+03,  ...,  1.8613e+02,\n",
      "           -9.7085e+01,  5.4932e+02],\n",
      "          [ 6.5238e+02, -1.3296e+02,  3.0373e+02,  ...,  4.5821e+02,\n",
      "           -1.5939e+02, -4.9861e+02]],\n",
      "\n",
      "         [[ 1.7959e+02, -4.7434e+02, -4.0799e+02,  ...,  1.4555e+03,\n",
      "            5.5930e+02,  7.1281e+02],\n",
      "          [-7.5991e+02,  8.6730e+02,  1.5682e+01,  ..., -2.3311e+02,\n",
      "           -7.9121e+02, -2.8077e+02],\n",
      "          [ 2.4089e+02, -2.7542e+02, -5.2263e+02,  ...,  9.4958e+02,\n",
      "            2.9189e+02, -1.2104e+03],\n",
      "          ...,\n",
      "          [ 7.1382e+02, -5.8429e+02,  1.1082e+02,  ...,  8.0872e+02,\n",
      "           -2.2934e+02, -1.2323e+03],\n",
      "          [ 7.6098e+02, -6.1908e+02,  2.8552e+02,  ..., -1.7178e+02,\n",
      "           -8.1481e+02, -1.1928e+03],\n",
      "          [-9.2770e+02,  8.3424e+02, -2.7813e+02,  ...,  8.9320e+00,\n",
      "           -6.9589e+02, -7.9090e+01]]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[ 3.4218e+02, -8.6976e+02,  3.6668e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.2197e+03,  9.5816e+01,  3.3452e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-3.9936e+02,  3.9866e+02,  1.0080e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-8.2364e+02, -2.2045e+02,  5.7169e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.8334e+02, -4.1499e+02,  2.2163e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-4.4546e+02, -1.4030e+02,  6.2033e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 1.6142e+02,  2.1670e+02,  2.9084e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.0911e+02, -1.3317e+02, -4.5254e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.7861e+02, -4.5345e+02,  1.9414e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 8.9539e+02,  1.4757e+02, -3.0913e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 9.9351e+02,  6.3356e+01, -1.2240e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.0284e+03,  9.0634e+01, -3.3612e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-2.9516e+02,  5.9295e+02,  4.0545e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.7080e+02,  1.0050e+02,  1.0152e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.9117e+02, -3.0927e+02, -2.1242e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-4.4971e+02, -1.7594e+02,  2.6345e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-6.7543e+02, -1.8746e+02,  4.6411e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-4.3747e+02, -1.0438e+02,  4.5446e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1061e+02,  2.9906e+02,  9.4297e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.1285e+02, -1.1443e+02, -2.1469e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.3643e+02,  1.5105e+03, -6.1317e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 8.3992e+01,  4.5260e+02,  1.1021e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 3.2910e+02,  4.6341e+02,  4.6066e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.4124e+02,  4.7980e+02, -2.3059e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 9.8267e+01,  1.0094e+03, -1.0955e+03,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 7.1445e+01,  4.2374e+02,  5.2515e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 4.3379e+02, -6.0948e+02, -5.5811e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-5.0384e+02,  8.3337e+02, -5.0413e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-7.3357e+02,  7.1496e+02, -7.6455e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-6.0850e+02,  4.0839e+02, -8.3264e+02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-2.9636e+02, -1.9687e+02,  6.1549e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-7.5045e+02,  1.0902e+03, -2.1297e+02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-9.4091e+02,  3.7661e+02, -8.4052e+01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 2.2157e+02, -7.8715e+02,  1.2538e+03,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.9209e+02, -4.7450e+02,  1.3106e+03,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.4853e+02, -5.5079e+02,  1.4661e+03,  ...,        -inf,\n",
      "                  -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1395e+02, -9.2655e+02, -5.7565e+01,  ...,  2.9651e+02,\n",
      "           -7.3549e+02, -7.2898e+02],\n",
      "          [-1.0786e+03, -3.2513e+01, -1.0779e+03,  ..., -5.8853e+02,\n",
      "           -1.8400e+02,  3.6196e+02],\n",
      "          [ 1.0304e+03, -8.7322e+02, -3.8980e+02,  ..., -4.2369e+02,\n",
      "            4.1440e+02, -2.4231e+02],\n",
      "          ...,\n",
      "          [-2.2308e+02, -1.4340e+03, -4.2642e+02,  ..., -1.3072e+03,\n",
      "            6.3081e+02, -7.1862e+01],\n",
      "          [ 8.6040e+02, -1.5548e+02,  2.8807e+02,  ..., -1.6388e+02,\n",
      "            1.0623e+03, -5.8511e+02],\n",
      "          [-5.9149e+02, -2.9463e+02, -6.0810e+02,  ..., -3.3882e+02,\n",
      "           -5.2203e+02, -5.4020e+00]],\n",
      "\n",
      "         [[-4.2043e+02,  4.5077e+02,  1.1635e+03,  ...,  6.0400e+02,\n",
      "           -2.5798e+02,  3.8241e+01],\n",
      "          [-2.4259e+02, -1.3059e+02, -2.3037e+02,  ..., -6.4703e+02,\n",
      "            2.4123e+02,  7.0558e+02],\n",
      "          [-8.0497e+02, -8.3345e+02, -3.3298e+02,  ...,  4.6661e+02,\n",
      "           -1.2122e+02, -9.2582e+02],\n",
      "          ...,\n",
      "          [-5.6490e+01, -1.7207e+02, -1.1182e+03,  ..., -6.1483e+02,\n",
      "            6.9552e+01,  2.8832e+02],\n",
      "          [-2.5920e+02,  1.8681e+02, -1.8339e+02,  ...,  6.1840e+02,\n",
      "            2.0889e+00, -6.8624e+02],\n",
      "          [ 4.1073e+02,  5.1179e+02,  2.8795e+01,  ...,  2.3313e+02,\n",
      "            9.9293e+02,  5.2721e+02]],\n",
      "\n",
      "         [[-5.8166e+01,  9.4666e+02,  7.3621e+01,  ...,  1.0152e+02,\n",
      "           -9.2178e+02,  1.3250e+02],\n",
      "          [-3.4675e+02,  1.2949e+00,  5.8701e+01,  ...,  8.2133e+02,\n",
      "           -1.0451e+03,  5.9005e+02],\n",
      "          [-5.0862e+02,  2.7825e+02,  1.3851e+02,  ..., -1.3049e+03,\n",
      "           -2.7396e+02,  1.4996e+02],\n",
      "          ...,\n",
      "          [-1.1856e+02, -7.7038e+02, -2.0630e+02,  ...,  4.4276e+02,\n",
      "            3.7406e+02,  6.5805e+02],\n",
      "          [-6.1899e+02,  3.5322e+02,  1.9219e+02,  ..., -1.6310e+03,\n",
      "            1.8652e+03, -2.3878e+02],\n",
      "          [ 5.9423e+02, -7.0770e+02, -1.2088e+02,  ...,  1.5620e+03,\n",
      "            1.0650e+03,  8.3290e+02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.3452e+02, -1.5101e+02,  3.2515e+02,  ...,  6.6688e+01,\n",
      "            4.5363e+01,  1.1061e+02],\n",
      "          [-5.3735e+02,  6.2385e+02,  6.2291e+02,  ...,  2.4231e+02,\n",
      "            8.6995e+02, -6.6978e+02],\n",
      "          [ 5.0686e+01,  3.4170e+02, -6.8151e+02,  ..., -7.8994e+02,\n",
      "            3.8432e+02,  2.0204e+01],\n",
      "          ...,\n",
      "          [-3.9039e+02, -7.2712e+02,  3.1581e+02,  ..., -5.2795e+02,\n",
      "            5.8145e+02, -4.9554e+02],\n",
      "          [ 8.0961e+02, -3.8641e+02,  1.3288e+02,  ..., -7.4137e+02,\n",
      "           -1.3921e+02,  1.0953e+02],\n",
      "          [ 2.8697e+01,  1.0901e+03, -1.9392e+02,  ...,  1.7334e+02,\n",
      "            2.1969e+00, -2.6716e+02]],\n",
      "\n",
      "         [[ 4.0026e+02,  6.4654e+02, -1.6552e+02,  ...,  4.4782e+02,\n",
      "            1.5644e+02, -4.3960e+02],\n",
      "          [-1.0859e+02,  8.4824e+02,  6.1296e+02,  ...,  5.8922e+02,\n",
      "            3.6573e+02, -2.5665e+02],\n",
      "          [ 6.6824e+02,  6.0980e+01, -1.2653e+02,  ..., -3.1376e+02,\n",
      "           -2.0896e+02,  1.1052e+01],\n",
      "          ...,\n",
      "          [-3.2839e+02, -2.4641e+02, -5.9611e+01,  ...,  3.7329e+02,\n",
      "            4.1883e+02, -6.2804e+02],\n",
      "          [-6.5011e+01, -1.3560e+03, -1.0535e+03,  ...,  1.8613e+02,\n",
      "           -9.7085e+01,  5.4932e+02],\n",
      "          [ 6.5238e+02, -1.3296e+02,  3.0373e+02,  ...,  4.5821e+02,\n",
      "           -1.5939e+02, -4.9861e+02]],\n",
      "\n",
      "         [[ 1.7959e+02, -4.7434e+02, -4.0799e+02,  ...,  1.4555e+03,\n",
      "            5.5930e+02,  7.1281e+02],\n",
      "          [-7.5991e+02,  8.6730e+02,  1.5682e+01,  ..., -2.3311e+02,\n",
      "           -7.9121e+02, -2.8077e+02],\n",
      "          [ 2.4089e+02, -2.7542e+02, -5.2263e+02,  ...,  9.4958e+02,\n",
      "            2.9189e+02, -1.2104e+03],\n",
      "          ...,\n",
      "          [ 7.1382e+02, -5.8429e+02,  1.1082e+02,  ...,  8.0872e+02,\n",
      "           -2.2934e+02, -1.2323e+03],\n",
      "          [ 7.6098e+02, -6.1908e+02,  2.8552e+02,  ..., -1.7178e+02,\n",
      "           -8.1481e+02, -1.1928e+03],\n",
      "          [-9.2770e+02,  8.3424e+02, -2.7813e+02,  ...,  8.9320e+00,\n",
      "           -6.9589e+02, -7.9090e+01]]]], grad_fn=<MaskedFillBackward0>)\n",
      "score_shape torch.Size([2, 8, 20, 20])\n",
      "tensor([[[[ 1.3314, -0.5393,  0.4559,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0132,  1.2542, -0.3035,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7513,  0.1669,  0.2613,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 1.3199,  0.5343,  1.3788,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4293,  0.2181,  0.4262,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4221,  1.2552,  0.9120,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0797,  0.3487, -0.7380,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.2149, -1.2454, -0.7521,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7755, -0.0188, -3.2325,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.5190, -1.1740, -1.0683,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1458, -0.7216, -1.1653,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3437, -1.1484, -0.7036,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0866, -1.1520, -1.3538,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.3923,  0.0476,  0.4961,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3828, -0.4357, -0.4407,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.3130,  0.5291,  0.0159,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3099,  0.3048, -0.4968,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5143,  1.3184, -0.4008,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7761, -0.1776, -1.5969,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.6149,  0.6661, -0.1572,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1176,  1.2811, -0.6348,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.1260, -1.2383, -0.5269,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0089, -1.3203, -0.5300,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5035, -0.8390, -0.3459,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3714, -0.8549,  0.0201,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0043, -0.1252, -0.5605,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.1037,  0.1368,  1.1434,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.3492,  0.5379, -0.9650,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0903,  0.3298, -2.2936,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4008,  0.0412, -2.5254,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 1.3226, -1.4029, -1.3419,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.2840,  0.7685, -1.5451,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5479,  1.8908,  0.7772,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.9260,  1.2033,  0.6019,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7272, -0.6543,  0.1667,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.5150,  0.4717,  1.0739,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9213, -0.3422,  0.5347,  ...,  1.6285,  0.4852,  0.8671],\n",
      "          [ 1.0814,  1.0869,  1.2278,  ...,  0.7458,  1.4284,  0.5850],\n",
      "          [-0.7076,  0.6833, -0.5233,  ...,  0.6200, -1.3786,  2.0797],\n",
      "          ...,\n",
      "          [-0.0069,  0.3977, -0.6454,  ..., -0.3055,  0.0086, -0.1637],\n",
      "          [ 1.2192,  0.5869,  0.5662,  ...,  0.0894, -1.2047,  1.1212],\n",
      "          [ 0.1622,  0.2997,  0.3674,  ..., -0.0819,  0.6927,  0.4243]],\n",
      "\n",
      "         [[-0.7986, -0.8350, -1.7572,  ...,  0.4178,  0.2461, -0.6589],\n",
      "          [ 1.7180, -1.4614,  0.0691,  ..., -0.3565, -0.5176,  0.3878],\n",
      "          [ 1.7135, -0.9868, -0.2950,  ..., -0.6582, -2.4889, -1.4127],\n",
      "          ...,\n",
      "          [ 0.6514, -0.4019, -1.0344,  ..., -1.2910, -1.1802, -1.5298],\n",
      "          [ 0.2583, -0.5471, -0.5618,  ..., -1.4767, -0.9220, -0.3418],\n",
      "          [-0.5718,  0.7439, -0.1911,  ..., -0.7826, -0.8518,  1.2626]],\n",
      "\n",
      "         [[-0.6023,  0.1563, -1.0705,  ..., -0.9880,  0.7308,  0.6073],\n",
      "          [-1.4124,  0.1955, -0.1911,  ...,  0.6883, -0.3744, -0.9856],\n",
      "          [-0.9929,  0.1794, -0.8112,  ..., -0.0841, -1.1514,  0.1862],\n",
      "          ...,\n",
      "          [-0.0738, -0.4419, -0.1900,  ..., -2.2979, -0.7966, -0.7051],\n",
      "          [-0.0498, -0.2789, -1.2832,  ..., -0.2158,  0.0792,  1.4966],\n",
      "          [-1.1328, -0.1076, -0.8514,  ..., -0.1511,  0.3016, -0.2935]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0352, -0.3526, -0.5572,  ...,  0.2314, -0.8647, -0.2341],\n",
      "          [-0.5575,  0.8174, -1.0605,  ...,  0.1831,  0.6413,  0.6767],\n",
      "          [-0.1621,  0.1643, -0.0635,  ...,  0.1638,  0.3723,  1.1257],\n",
      "          ...,\n",
      "          [ 0.2997,  0.4429, -0.7601,  ..., -1.7185, -0.8927, -1.0580],\n",
      "          [-0.6506,  0.9883, -1.5213,  ...,  0.0531, -1.2528, -0.1151],\n",
      "          [ 0.1080,  1.1872, -0.5743,  ...,  0.1718, -0.9303,  0.1476]],\n",
      "\n",
      "         [[-0.6231, -0.5673, -0.9328,  ..., -3.2163, -0.2559, -0.4999],\n",
      "          [-0.8126, -0.2288, -1.5310,  ...,  0.2945,  0.2795,  0.3673],\n",
      "          [-0.2424, -0.2809, -1.2647,  ...,  0.0465, -0.1775,  0.7043],\n",
      "          ...,\n",
      "          [-0.8044,  1.2446, -0.4100,  ..., -0.9315, -1.6685, -0.5247],\n",
      "          [-0.5618, -0.6903, -2.3962,  ..., -1.2412, -0.6398, -0.3222],\n",
      "          [ 0.5153, -0.0073, -0.6700,  ..., -2.7191,  0.0556, -0.7355]],\n",
      "\n",
      "         [[ 1.1952, -1.0048, -0.1447,  ..., -0.1280,  0.2409, -0.0142],\n",
      "          [-0.0742,  0.6658, -0.3599,  ...,  0.9458,  1.3145,  0.8075],\n",
      "          [ 0.9949,  1.9414,  1.5435,  ..., -0.2361,  0.0221,  0.1258],\n",
      "          ...,\n",
      "          [ 0.5014,  0.4707,  0.9891,  ..., -0.2867, -0.6011, -0.6847],\n",
      "          [ 0.1424, -1.0659,  1.2246,  ...,  0.7612,  0.7780, -0.1166],\n",
      "          [-0.0450, -0.8430, -0.2231,  ...,  0.9092,  2.1051,  1.9231]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[ 1.3314, -0.5393,  0.4559,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0132,  1.2542, -0.3035,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7513,  0.1669,  0.2613,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 1.3199,  0.5343,  1.3788,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4293,  0.2181,  0.4262,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4221,  1.2552,  0.9120,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0797,  0.3487, -0.7380,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.2149, -1.2454, -0.7521,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7755, -0.0188, -3.2325,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.5190, -1.1740, -1.0683,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1458, -0.7216, -1.1653,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3437, -1.1484, -0.7036,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0866, -1.1520, -1.3538,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.3923,  0.0476,  0.4961,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3828, -0.4357, -0.4407,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.3130,  0.5291,  0.0159,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3099,  0.3048, -0.4968,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5143,  1.3184, -0.4008,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7761, -0.1776, -1.5969,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.6149,  0.6661, -0.1572,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1176,  1.2811, -0.6348,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.1260, -1.2383, -0.5269,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0089, -1.3203, -0.5300,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5035, -0.8390, -0.3459,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3714, -0.8549,  0.0201,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0043, -0.1252, -0.5605,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.1037,  0.1368,  1.1434,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.3492,  0.5379, -0.9650,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0903,  0.3298, -2.2936,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4008,  0.0412, -2.5254,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 1.3226, -1.4029, -1.3419,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.2840,  0.7685, -1.5451,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5479,  1.8908,  0.7772,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.9260,  1.2033,  0.6019,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7272, -0.6543,  0.1667,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.5150,  0.4717,  1.0739,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9213, -0.3422,  0.5347,  ...,  1.6285,  0.4852,  0.8671],\n",
      "          [ 1.0814,  1.0869,  1.2278,  ...,  0.7458,  1.4284,  0.5850],\n",
      "          [-0.7076,  0.6833, -0.5233,  ...,  0.6200, -1.3786,  2.0797],\n",
      "          ...,\n",
      "          [-0.0069,  0.3977, -0.6454,  ..., -0.3055,  0.0086, -0.1637],\n",
      "          [ 1.2192,  0.5869,  0.5662,  ...,  0.0894, -1.2047,  1.1212],\n",
      "          [ 0.1622,  0.2997,  0.3674,  ..., -0.0819,  0.6927,  0.4243]],\n",
      "\n",
      "         [[-0.7986, -0.8350, -1.7572,  ...,  0.4178,  0.2461, -0.6589],\n",
      "          [ 1.7180, -1.4614,  0.0691,  ..., -0.3565, -0.5176,  0.3878],\n",
      "          [ 1.7135, -0.9868, -0.2950,  ..., -0.6582, -2.4889, -1.4127],\n",
      "          ...,\n",
      "          [ 0.6514, -0.4019, -1.0344,  ..., -1.2910, -1.1802, -1.5298],\n",
      "          [ 0.2583, -0.5471, -0.5618,  ..., -1.4767, -0.9220, -0.3418],\n",
      "          [-0.5718,  0.7439, -0.1911,  ..., -0.7826, -0.8518,  1.2626]],\n",
      "\n",
      "         [[-0.6023,  0.1563, -1.0705,  ..., -0.9880,  0.7308,  0.6073],\n",
      "          [-1.4124,  0.1955, -0.1911,  ...,  0.6883, -0.3744, -0.9856],\n",
      "          [-0.9929,  0.1794, -0.8112,  ..., -0.0841, -1.1514,  0.1862],\n",
      "          ...,\n",
      "          [-0.0738, -0.4419, -0.1900,  ..., -2.2979, -0.7966, -0.7051],\n",
      "          [-0.0498, -0.2789, -1.2832,  ..., -0.2158,  0.0792,  1.4966],\n",
      "          [-1.1328, -0.1076, -0.8514,  ..., -0.1511,  0.3016, -0.2935]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0352, -0.3526, -0.5572,  ...,  0.2314, -0.8647, -0.2341],\n",
      "          [-0.5575,  0.8174, -1.0605,  ...,  0.1831,  0.6413,  0.6767],\n",
      "          [-0.1621,  0.1643, -0.0635,  ...,  0.1638,  0.3723,  1.1257],\n",
      "          ...,\n",
      "          [ 0.2997,  0.4429, -0.7601,  ..., -1.7185, -0.8927, -1.0580],\n",
      "          [-0.6506,  0.9883, -1.5213,  ...,  0.0531, -1.2528, -0.1151],\n",
      "          [ 0.1080,  1.1872, -0.5743,  ...,  0.1718, -0.9303,  0.1476]],\n",
      "\n",
      "         [[-0.6231, -0.5673, -0.9328,  ..., -3.2163, -0.2559, -0.4999],\n",
      "          [-0.8126, -0.2288, -1.5310,  ...,  0.2945,  0.2795,  0.3673],\n",
      "          [-0.2424, -0.2809, -1.2647,  ...,  0.0465, -0.1775,  0.7043],\n",
      "          ...,\n",
      "          [-0.8044,  1.2446, -0.4100,  ..., -0.9315, -1.6685, -0.5247],\n",
      "          [-0.5618, -0.6903, -2.3962,  ..., -1.2412, -0.6398, -0.3222],\n",
      "          [ 0.5153, -0.0073, -0.6700,  ..., -2.7191,  0.0556, -0.7355]],\n",
      "\n",
      "         [[ 1.1952, -1.0048, -0.1447,  ..., -0.1280,  0.2409, -0.0142],\n",
      "          [-0.0742,  0.6658, -0.3599,  ...,  0.9458,  1.3145,  0.8075],\n",
      "          [ 0.9949,  1.9414,  1.5435,  ..., -0.2361,  0.0221,  0.1258],\n",
      "          ...,\n",
      "          [ 0.5014,  0.4707,  0.9891,  ..., -0.2867, -0.6011, -0.6847],\n",
      "          [ 0.1424, -1.0659,  1.2246,  ...,  0.7612,  0.7780, -0.1166],\n",
      "          [-0.0450, -0.8430, -0.2231,  ...,  0.9092,  2.1051,  1.9231]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "score_shape torch.Size([2, 8, 20, 20])\n",
      "tensor([[[[-1.1565e+00, -6.1715e-01, -2.9474e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 7.6746e-01, -1.1592e-01,  1.1378e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-7.4618e-01,  1.4789e+00, -2.6270e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 2.4185e-01,  5.9422e-01,  1.2060e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.6036e-01,  8.3535e-02,  1.0387e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.1128e-01,  8.8492e-01,  1.7336e+00,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-6.2182e-01, -4.3414e-01, -2.5921e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.5549e+00, -1.0072e+00,  1.0374e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.1033e-01, -8.8149e-01, -7.3063e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 2.0014e-01,  3.8862e-01,  2.0529e-02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.1228e-01, -6.1646e-01,  2.0914e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.4711e-02, -5.5231e-01, -2.5198e-02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-1.2941e-03, -6.8776e-01, -7.8852e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-3.3944e-01,  5.3292e-01, -1.0640e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.7693e+00,  4.8114e-02, -3.1868e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-1.7384e+00, -1.0059e-01, -1.2584e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.0506e+00,  2.7783e-01, -1.9808e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.4862e+00,  7.0208e-01, -1.0863e+00,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5552e-01,  4.4746e-01,  3.3862e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.0067e+00, -7.1962e-01, -3.9488e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 7.1404e-01,  6.1093e-01, -1.0735e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 5.1443e-02,  5.7580e-02, -1.7453e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-4.8738e-01, -7.7891e-01, -2.4892e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-3.1692e-01, -1.3319e-02, -4.7139e-01,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 5.1277e-01,  1.2115e+00,  1.6383e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.3504e+00,  4.9170e-01,  6.1622e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.0945e-01, -2.3775e-01, -1.2753e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 9.0302e-01,  2.6353e-01,  9.3290e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.2387e+00, -9.9236e-02,  9.3882e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.7604e+00, -1.3191e+00,  1.0114e+00,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 2.6658e-01,  1.8206e-01,  9.5693e-02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.1459e+00,  1.1072e+00,  3.5479e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 4.8025e-01,  1.0975e-01,  4.5936e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-3.2278e-02, -7.5166e-01,  1.3927e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 3.1569e-01, -1.7796e+00, -2.0702e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 3.4328e-01, -1.0128e+00, -4.6052e-01,  ...,        -inf,\n",
      "                  -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.3288e+00, -4.4267e-02, -7.2910e-01,  ..., -2.2031e+00,\n",
      "           -5.6364e-01, -2.0054e+00],\n",
      "          [ 2.1183e-01, -5.5675e-01, -1.1930e-02,  ...,  1.7195e-01,\n",
      "           -3.9244e-01, -6.4457e-01],\n",
      "          [ 4.7241e-01, -7.1951e-01, -3.7000e-01,  ...,  4.2364e-01,\n",
      "            1.8844e+00,  2.6540e+00],\n",
      "          ...,\n",
      "          [ 8.1209e-01,  3.1847e-01,  5.1813e-02,  ...,  5.1991e-02,\n",
      "           -1.3644e-01,  8.2611e-01],\n",
      "          [-4.7462e-02, -4.7341e-01, -6.3732e-01,  ..., -4.4005e-01,\n",
      "            1.5045e-01,  8.8162e-01],\n",
      "          [-3.9805e-01, -3.7511e-01, -6.9238e-01,  ..., -2.4158e-01,\n",
      "            1.1088e+00,  4.8756e-01]],\n",
      "\n",
      "         [[-3.8853e-01, -4.9437e-01,  3.8947e-01,  ..., -1.3664e-01,\n",
      "            5.8436e-02, -1.4762e+00],\n",
      "          [-2.0700e+00, -5.6021e-01, -2.1029e-01,  ...,  9.3540e-01,\n",
      "            2.8158e-02, -6.9339e-01],\n",
      "          [-7.9135e-01,  2.7729e-01, -1.6478e-01,  ...,  2.7472e-01,\n",
      "            2.0756e-01, -1.0533e+00],\n",
      "          ...,\n",
      "          [-6.2399e-01, -8.2945e-01, -8.1139e-01,  ..., -7.1363e-01,\n",
      "           -7.5795e-02, -1.6344e+00],\n",
      "          [-2.1383e+00, -4.7517e-01,  6.7195e-02,  ..., -2.6322e-01,\n",
      "            1.4053e-01, -1.2545e+00],\n",
      "          [-1.6934e+00,  1.1049e-01, -2.3227e-01,  ..., -1.5547e+00,\n",
      "            1.1099e-01, -1.9733e+00]],\n",
      "\n",
      "         [[-1.2040e+00,  2.7695e-01, -4.8121e-01,  ..., -2.0549e-01,\n",
      "           -2.1965e+00, -1.1220e+00],\n",
      "          [-2.8550e-01,  9.8434e-01, -1.1524e+00,  ..., -2.9687e-01,\n",
      "           -1.7339e+00,  9.2223e-01],\n",
      "          [-9.6895e-01,  3.1960e-01, -6.6932e-01,  ..., -4.9782e-01,\n",
      "           -6.2936e-01, -1.6924e+00],\n",
      "          ...,\n",
      "          [-2.7737e+00, -1.1308e+00, -9.6575e-01,  ..., -1.6412e+00,\n",
      "           -1.6132e+00,  2.9447e-01],\n",
      "          [-2.8128e+00, -1.9393e-01, -2.9336e+00,  ..., -8.1594e-01,\n",
      "           -1.1163e+00, -7.3594e-01],\n",
      "          [-6.0766e-01,  1.6910e+00, -1.4371e+00,  ...,  5.1360e-01,\n",
      "            6.2293e-01,  3.9462e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9623e-01,  8.4448e-01,  1.9916e+00,  ...,  1.2295e+00,\n",
      "            1.0347e+00,  3.5878e-01],\n",
      "          [-1.1884e+00, -2.3187e-01,  8.6879e-01,  ..., -1.0512e+00,\n",
      "            2.2856e-01, -7.6294e-01],\n",
      "          [-9.0339e-02, -6.6690e-01, -9.8921e-01,  ...,  1.1207e+00,\n",
      "            1.4193e-01, -5.8891e-01],\n",
      "          ...,\n",
      "          [-2.9688e-01,  4.6619e-01,  2.9697e-01,  ...,  7.4547e-01,\n",
      "            5.5619e-01, -2.6660e-01],\n",
      "          [-1.3412e+00, -1.4113e-01, -5.1668e-01,  ..., -1.2132e-01,\n",
      "            3.1426e-02, -4.3880e-02],\n",
      "          [-4.2970e-01, -2.4851e-01, -4.1813e-02,  ...,  7.2035e-02,\n",
      "            8.7460e-01, -1.8829e-01]],\n",
      "\n",
      "         [[ 3.6772e-01,  2.6328e+00,  1.0772e+00,  ...,  1.3629e+00,\n",
      "            6.1797e-01,  1.7027e-01],\n",
      "          [ 2.1124e+00, -1.1045e+00,  3.8344e-01,  ...,  2.1932e+00,\n",
      "            2.2429e+00,  1.1428e+00],\n",
      "          [-1.6128e-01, -3.4074e-01, -6.3787e-01,  ...,  2.4998e-02,\n",
      "            2.3942e-01, -2.3988e-01],\n",
      "          ...,\n",
      "          [ 1.5744e-01, -2.0168e-01, -1.9817e-01,  ...,  5.6368e-02,\n",
      "            4.5322e-01, -1.9873e+00],\n",
      "          [ 1.4130e+00,  3.5876e-01,  7.3922e-01,  ...,  2.9999e-01,\n",
      "            1.4597e+00, -6.6491e-01],\n",
      "          [ 1.9733e+00,  8.2982e-01,  9.8096e-01,  ...,  7.0023e-01,\n",
      "            1.9290e+00, -1.3499e+00]],\n",
      "\n",
      "         [[ 6.4930e-01, -1.5073e-02, -3.3840e-02,  ..., -3.4083e-01,\n",
      "           -2.8030e-01, -3.6085e-01],\n",
      "          [ 1.1481e+00,  7.8202e-01,  8.5603e-01,  ...,  1.9369e+00,\n",
      "            2.8804e+00,  1.4730e+00],\n",
      "          [-1.0002e+00, -9.1189e-03,  5.6993e-01,  ...,  1.0600e+00,\n",
      "            1.3983e+00, -5.8163e-01],\n",
      "          ...,\n",
      "          [ 9.2061e-01,  1.0014e-01,  4.1862e-02,  ..., -1.8015e-01,\n",
      "            1.0897e+00, -7.9637e-01],\n",
      "          [-3.2193e-01, -1.7257e+00, -6.4207e-02,  ..., -1.4591e+00,\n",
      "            8.5272e-01, -3.8709e-01],\n",
      "          [ 2.4839e-02, -1.4297e+00, -7.0217e-01,  ..., -5.9649e-02,\n",
      "            9.4379e-01, -8.7015e-02]]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[-1.1565e+00, -6.1715e-01, -2.9474e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 7.6746e-01, -1.1592e-01,  1.1378e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-7.4618e-01,  1.4789e+00, -2.6270e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 2.4185e-01,  5.9422e-01,  1.2060e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.6036e-01,  8.3535e-02,  1.0387e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.1128e-01,  8.8492e-01,  1.7336e+00,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-6.2182e-01, -4.3414e-01, -2.5921e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.5549e+00, -1.0072e+00,  1.0374e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.1033e-01, -8.8149e-01, -7.3063e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 2.0014e-01,  3.8862e-01,  2.0529e-02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.1228e-01, -6.1646e-01,  2.0914e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.4711e-02, -5.5231e-01, -2.5198e-02,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[-1.2941e-03, -6.8776e-01, -7.8852e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-3.3944e-01,  5.3292e-01, -1.0640e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.7693e+00,  4.8114e-02, -3.1868e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-1.7384e+00, -1.0059e-01, -1.2584e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.0506e+00,  2.7783e-01, -1.9808e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-1.4862e+00,  7.0208e-01, -1.0863e+00,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5552e-01,  4.4746e-01,  3.3862e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-2.0067e+00, -7.1962e-01, -3.9488e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 7.1404e-01,  6.1093e-01, -1.0735e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 5.1443e-02,  5.7580e-02, -1.7453e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-4.8738e-01, -7.7891e-01, -2.4892e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-3.1692e-01, -1.3319e-02, -4.7139e-01,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 5.1277e-01,  1.2115e+00,  1.6383e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 2.3504e+00,  4.9170e-01,  6.1622e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [-5.0945e-01, -2.3775e-01, -1.2753e+00,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [ 9.0302e-01,  2.6353e-01,  9.3290e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.2387e+00, -9.9236e-02,  9.3882e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.7604e+00, -1.3191e+00,  1.0114e+00,  ...,        -inf,\n",
      "                  -inf,        -inf]],\n",
      "\n",
      "         [[ 2.6658e-01,  1.8206e-01,  9.5693e-02,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 1.1459e+00,  1.1072e+00,  3.5479e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 4.8025e-01,  1.0975e-01,  4.5936e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          ...,\n",
      "          [-3.2278e-02, -7.5166e-01,  1.3927e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 3.1569e-01, -1.7796e+00, -2.0702e-01,  ...,        -inf,\n",
      "                  -inf,        -inf],\n",
      "          [ 3.4328e-01, -1.0128e+00, -4.6052e-01,  ...,        -inf,\n",
      "                  -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.3288e+00, -4.4267e-02, -7.2910e-01,  ..., -2.2031e+00,\n",
      "           -5.6364e-01, -2.0054e+00],\n",
      "          [ 2.1183e-01, -5.5675e-01, -1.1930e-02,  ...,  1.7195e-01,\n",
      "           -3.9244e-01, -6.4457e-01],\n",
      "          [ 4.7241e-01, -7.1951e-01, -3.7000e-01,  ...,  4.2364e-01,\n",
      "            1.8844e+00,  2.6540e+00],\n",
      "          ...,\n",
      "          [ 8.1209e-01,  3.1847e-01,  5.1813e-02,  ...,  5.1991e-02,\n",
      "           -1.3644e-01,  8.2611e-01],\n",
      "          [-4.7462e-02, -4.7341e-01, -6.3732e-01,  ..., -4.4005e-01,\n",
      "            1.5045e-01,  8.8162e-01],\n",
      "          [-3.9805e-01, -3.7511e-01, -6.9238e-01,  ..., -2.4158e-01,\n",
      "            1.1088e+00,  4.8756e-01]],\n",
      "\n",
      "         [[-3.8853e-01, -4.9437e-01,  3.8947e-01,  ..., -1.3664e-01,\n",
      "            5.8436e-02, -1.4762e+00],\n",
      "          [-2.0700e+00, -5.6021e-01, -2.1029e-01,  ...,  9.3540e-01,\n",
      "            2.8158e-02, -6.9339e-01],\n",
      "          [-7.9135e-01,  2.7729e-01, -1.6478e-01,  ...,  2.7472e-01,\n",
      "            2.0756e-01, -1.0533e+00],\n",
      "          ...,\n",
      "          [-6.2399e-01, -8.2945e-01, -8.1139e-01,  ..., -7.1363e-01,\n",
      "           -7.5795e-02, -1.6344e+00],\n",
      "          [-2.1383e+00, -4.7517e-01,  6.7195e-02,  ..., -2.6322e-01,\n",
      "            1.4053e-01, -1.2545e+00],\n",
      "          [-1.6934e+00,  1.1049e-01, -2.3227e-01,  ..., -1.5547e+00,\n",
      "            1.1099e-01, -1.9733e+00]],\n",
      "\n",
      "         [[-1.2040e+00,  2.7695e-01, -4.8121e-01,  ..., -2.0549e-01,\n",
      "           -2.1965e+00, -1.1220e+00],\n",
      "          [-2.8550e-01,  9.8434e-01, -1.1524e+00,  ..., -2.9687e-01,\n",
      "           -1.7339e+00,  9.2223e-01],\n",
      "          [-9.6895e-01,  3.1960e-01, -6.6932e-01,  ..., -4.9782e-01,\n",
      "           -6.2936e-01, -1.6924e+00],\n",
      "          ...,\n",
      "          [-2.7737e+00, -1.1308e+00, -9.6575e-01,  ..., -1.6412e+00,\n",
      "           -1.6132e+00,  2.9447e-01],\n",
      "          [-2.8128e+00, -1.9393e-01, -2.9336e+00,  ..., -8.1594e-01,\n",
      "           -1.1163e+00, -7.3594e-01],\n",
      "          [-6.0766e-01,  1.6910e+00, -1.4371e+00,  ...,  5.1360e-01,\n",
      "            6.2293e-01,  3.9462e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9623e-01,  8.4448e-01,  1.9916e+00,  ...,  1.2295e+00,\n",
      "            1.0347e+00,  3.5878e-01],\n",
      "          [-1.1884e+00, -2.3187e-01,  8.6879e-01,  ..., -1.0512e+00,\n",
      "            2.2856e-01, -7.6294e-01],\n",
      "          [-9.0339e-02, -6.6690e-01, -9.8921e-01,  ...,  1.1207e+00,\n",
      "            1.4193e-01, -5.8891e-01],\n",
      "          ...,\n",
      "          [-2.9688e-01,  4.6619e-01,  2.9697e-01,  ...,  7.4547e-01,\n",
      "            5.5619e-01, -2.6660e-01],\n",
      "          [-1.3412e+00, -1.4113e-01, -5.1668e-01,  ..., -1.2132e-01,\n",
      "            3.1426e-02, -4.3880e-02],\n",
      "          [-4.2970e-01, -2.4851e-01, -4.1813e-02,  ...,  7.2035e-02,\n",
      "            8.7460e-01, -1.8829e-01]],\n",
      "\n",
      "         [[ 3.6772e-01,  2.6328e+00,  1.0772e+00,  ...,  1.3629e+00,\n",
      "            6.1797e-01,  1.7027e-01],\n",
      "          [ 2.1124e+00, -1.1045e+00,  3.8344e-01,  ...,  2.1932e+00,\n",
      "            2.2429e+00,  1.1428e+00],\n",
      "          [-1.6128e-01, -3.4074e-01, -6.3787e-01,  ...,  2.4998e-02,\n",
      "            2.3942e-01, -2.3988e-01],\n",
      "          ...,\n",
      "          [ 1.5744e-01, -2.0168e-01, -1.9817e-01,  ...,  5.6368e-02,\n",
      "            4.5322e-01, -1.9873e+00],\n",
      "          [ 1.4130e+00,  3.5876e-01,  7.3922e-01,  ...,  2.9999e-01,\n",
      "            1.4597e+00, -6.6491e-01],\n",
      "          [ 1.9733e+00,  8.2982e-01,  9.8096e-01,  ...,  7.0023e-01,\n",
      "            1.9290e+00, -1.3499e+00]],\n",
      "\n",
      "         [[ 6.4930e-01, -1.5073e-02, -3.3840e-02,  ..., -3.4083e-01,\n",
      "           -2.8030e-01, -3.6085e-01],\n",
      "          [ 1.1481e+00,  7.8202e-01,  8.5603e-01,  ...,  1.9369e+00,\n",
      "            2.8804e+00,  1.4730e+00],\n",
      "          [-1.0002e+00, -9.1189e-03,  5.6993e-01,  ...,  1.0600e+00,\n",
      "            1.3983e+00, -5.8163e-01],\n",
      "          ...,\n",
      "          [ 9.2061e-01,  1.0014e-01,  4.1862e-02,  ..., -1.8015e-01,\n",
      "            1.0897e+00, -7.9637e-01],\n",
      "          [-3.2193e-01, -1.7257e+00, -6.4207e-02,  ..., -1.4591e+00,\n",
      "            8.5272e-01, -3.8709e-01],\n",
      "          [ 2.4839e-02, -1.4297e+00, -7.0217e-01,  ..., -5.9649e-02,\n",
      "            9.4379e-01, -8.7015e-02]]]], grad_fn=<MaskedFillBackward0>)\n",
      "score_shape torch.Size([2, 8, 20, 20])\n",
      "tensor([[[[ 0.0699,  0.2820,  1.6680,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2208, -0.7614, -0.2152,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.0043,  0.4128,  1.1250,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.6298,  0.2679, -0.0906,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3584,  0.5228,  0.9388,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.9943,  1.6522,  0.4372,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2691, -0.0811,  1.1550,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0909,  0.3100,  1.7968,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5774,  0.6880,  1.1594,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.3000,  0.5435,  0.6734,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4545,  0.1230,  1.3460,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.8350,  0.1294,  1.7203,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2925,  1.1460,  0.7080,  ...,    -inf,    -inf,    -inf],\n",
      "          [-2.7211, -1.0888, -2.4541,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.8429, -0.8096, -0.8274,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-2.0090, -1.6140, -3.1090,  ...,    -inf,    -inf,    -inf],\n",
      "          [-3.0377, -1.4664, -3.0608,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.9354, -1.0600, -2.8580,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8417,  0.6205,  0.0395,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.9155,  0.8824,  0.3385,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4775,  1.0686,  0.4547,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 1.3139,  2.2886, -0.5269,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.2397,  2.4111, -0.6159,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.9625,  0.9424, -1.1335,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-2.3188, -0.2614, -2.0119,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2512,  0.4565,  0.4232,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0997,  0.9798, -1.3550,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.1775,  0.4309, -1.4827,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4006, -0.2551, -1.1163,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4270,  0.5910, -0.1450,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3618, -0.5743,  0.5654,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3165,  0.9702,  0.5026,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.1310,  0.8802, -0.3376,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.4440, -0.7666,  0.2934,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5252, -0.1243,  0.0817,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1533,  0.4063,  1.1043,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1803,  0.3339,  1.2502,  ..., -1.6549,  0.3088, -0.9338],\n",
      "          [ 1.4292,  0.1445,  1.2634,  ..., -0.1442,  0.5490, -0.6936],\n",
      "          [ 1.6497,  1.3223,  1.8730,  ..., -0.9160,  0.5492, -0.4575],\n",
      "          ...,\n",
      "          [ 0.3299,  1.1336,  1.6516,  ..., -0.4751, -0.0540,  0.1976],\n",
      "          [-0.1177,  0.3903,  1.9233,  ..., -0.1157,  0.3219,  0.0886],\n",
      "          [ 0.2201,  0.8157,  0.7954,  ..., -0.5758, -0.3333, -0.3228]],\n",
      "\n",
      "         [[-1.1855, -0.0213,  0.5854,  ..., -0.1281,  0.2417,  0.6497],\n",
      "          [-0.8261,  1.2152,  0.9613,  ..., -0.3696,  0.8807,  0.9834],\n",
      "          [-0.0182,  0.0467,  0.3797,  ...,  0.9626,  0.5687,  0.9014],\n",
      "          ...,\n",
      "          [-0.0608, -0.4786, -0.1863,  ..., -0.4893,  0.0790, -0.0551],\n",
      "          [-0.0845,  0.4484,  0.8086,  ...,  0.0706,  0.1274,  1.1018],\n",
      "          [ 0.0837,  0.1350,  0.7206,  ...,  1.0392,  1.2395,  1.3750]],\n",
      "\n",
      "         [[ 0.0766,  0.7368,  0.3065,  ...,  1.7131, -0.8943,  1.0498],\n",
      "          [-2.4262, -0.2977, -1.6752,  ..., -2.7188, -2.0374, -2.3233],\n",
      "          [-2.5680,  0.4765, -1.4407,  ..., -2.4539, -1.9944, -2.0718],\n",
      "          ...,\n",
      "          [-2.1603, -0.2007, -1.5265,  ..., -1.7013, -3.1209, -1.5215],\n",
      "          [-2.3133,  0.5483, -0.9186,  ..., -1.7511, -2.6797, -1.5974],\n",
      "          [-1.6443,  0.1984, -2.1155,  ..., -1.5353, -2.5466, -1.3196]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7531, -0.0653,  0.0227,  ...,  0.7399,  0.2961,  0.1970],\n",
      "          [ 0.0477, -0.4070,  0.0157,  ...,  0.8369, -0.1653,  1.6704],\n",
      "          [-0.3938,  0.4233, -0.0451,  ..., -0.0818,  0.2151,  0.6695],\n",
      "          ...,\n",
      "          [ 1.0995,  1.5729,  1.1526,  ...,  1.8957,  1.4315,  1.4628],\n",
      "          [ 0.3543,  0.1706,  0.1537,  ..., -0.2587, -0.5986, -1.6642],\n",
      "          [ 0.9213,  0.8045,  1.6737,  ...,  1.8575,  1.3332,  1.5298]],\n",
      "\n",
      "         [[-0.1560,  0.6506, -1.5512,  ..., -0.8006,  0.2788, -0.4923],\n",
      "          [ 0.5500,  0.0803,  0.2996,  ..., -1.0741,  1.0641, -0.1096],\n",
      "          [-0.0794,  0.8292, -0.6421,  ..., -0.7845,  0.5959,  0.0406],\n",
      "          ...,\n",
      "          [ 0.1065,  0.7129,  1.1746,  ..., -0.6454,  0.5519,  0.1046],\n",
      "          [ 0.9074,  0.3966,  0.8883,  ..., -1.1463,  0.7440, -0.4715],\n",
      "          [ 0.4909,  0.1612,  0.5821,  ..., -0.8936,  0.2658, -0.0655]],\n",
      "\n",
      "         [[ 0.2471, -0.6504, -0.6385,  ..., -0.3214, -0.5552,  0.0372],\n",
      "          [-0.0699,  0.3589,  0.3867,  ...,  0.3255,  0.1814,  0.8557],\n",
      "          [ 1.4972, -0.1520, -0.1657,  ...,  0.4608,  1.1114,  0.7806],\n",
      "          ...,\n",
      "          [ 0.6815, -0.5538,  0.4626,  ...,  0.5160,  1.0786,  0.2739],\n",
      "          [ 1.1937, -0.7059,  0.0758,  ...,  0.9809,  0.8781,  0.4082],\n",
      "          [-0.0300, -0.0597,  0.3127,  ...,  0.6107, -0.0274,  1.0842]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[ 0.0699,  0.2820,  1.6680,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2208, -0.7614, -0.2152,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.0043,  0.4128,  1.1250,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.6298,  0.2679, -0.0906,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3584,  0.5228,  0.9388,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.9943,  1.6522,  0.4372,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2691, -0.0811,  1.1550,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0909,  0.3100,  1.7968,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5774,  0.6880,  1.1594,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.3000,  0.5435,  0.6734,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4545,  0.1230,  1.3460,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.8350,  0.1294,  1.7203,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2925,  1.1460,  0.7080,  ...,    -inf,    -inf,    -inf],\n",
      "          [-2.7211, -1.0888, -2.4541,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.8429, -0.8096, -0.8274,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-2.0090, -1.6140, -3.1090,  ...,    -inf,    -inf,    -inf],\n",
      "          [-3.0377, -1.4664, -3.0608,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.9354, -1.0600, -2.8580,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8417,  0.6205,  0.0395,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.9155,  0.8824,  0.3385,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4775,  1.0686,  0.4547,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 1.3139,  2.2886, -0.5269,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.2397,  2.4111, -0.6159,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.9625,  0.9424, -1.1335,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-2.3188, -0.2614, -2.0119,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2512,  0.4565,  0.4232,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0997,  0.9798, -1.3550,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.1775,  0.4309, -1.4827,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4006, -0.2551, -1.1163,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4270,  0.5910, -0.1450,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3618, -0.5743,  0.5654,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3165,  0.9702,  0.5026,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.1310,  0.8802, -0.3376,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.4440, -0.7666,  0.2934,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5252, -0.1243,  0.0817,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1533,  0.4063,  1.1043,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1803,  0.3339,  1.2502,  ..., -1.6549,  0.3088, -0.9338],\n",
      "          [ 1.4292,  0.1445,  1.2634,  ..., -0.1442,  0.5490, -0.6936],\n",
      "          [ 1.6497,  1.3223,  1.8730,  ..., -0.9160,  0.5492, -0.4575],\n",
      "          ...,\n",
      "          [ 0.3299,  1.1336,  1.6516,  ..., -0.4751, -0.0540,  0.1976],\n",
      "          [-0.1177,  0.3903,  1.9233,  ..., -0.1157,  0.3219,  0.0886],\n",
      "          [ 0.2201,  0.8157,  0.7954,  ..., -0.5758, -0.3333, -0.3228]],\n",
      "\n",
      "         [[-1.1855, -0.0213,  0.5854,  ..., -0.1281,  0.2417,  0.6497],\n",
      "          [-0.8261,  1.2152,  0.9613,  ..., -0.3696,  0.8807,  0.9834],\n",
      "          [-0.0182,  0.0467,  0.3797,  ...,  0.9626,  0.5687,  0.9014],\n",
      "          ...,\n",
      "          [-0.0608, -0.4786, -0.1863,  ..., -0.4893,  0.0790, -0.0551],\n",
      "          [-0.0845,  0.4484,  0.8086,  ...,  0.0706,  0.1274,  1.1018],\n",
      "          [ 0.0837,  0.1350,  0.7206,  ...,  1.0392,  1.2395,  1.3750]],\n",
      "\n",
      "         [[ 0.0766,  0.7368,  0.3065,  ...,  1.7131, -0.8943,  1.0498],\n",
      "          [-2.4262, -0.2977, -1.6752,  ..., -2.7188, -2.0374, -2.3233],\n",
      "          [-2.5680,  0.4765, -1.4407,  ..., -2.4539, -1.9944, -2.0718],\n",
      "          ...,\n",
      "          [-2.1603, -0.2007, -1.5265,  ..., -1.7013, -3.1209, -1.5215],\n",
      "          [-2.3133,  0.5483, -0.9186,  ..., -1.7511, -2.6797, -1.5974],\n",
      "          [-1.6443,  0.1984, -2.1155,  ..., -1.5353, -2.5466, -1.3196]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7531, -0.0653,  0.0227,  ...,  0.7399,  0.2961,  0.1970],\n",
      "          [ 0.0477, -0.4070,  0.0157,  ...,  0.8369, -0.1653,  1.6704],\n",
      "          [-0.3938,  0.4233, -0.0451,  ..., -0.0818,  0.2151,  0.6695],\n",
      "          ...,\n",
      "          [ 1.0995,  1.5729,  1.1526,  ...,  1.8957,  1.4315,  1.4628],\n",
      "          [ 0.3543,  0.1706,  0.1537,  ..., -0.2587, -0.5986, -1.6642],\n",
      "          [ 0.9213,  0.8045,  1.6737,  ...,  1.8575,  1.3332,  1.5298]],\n",
      "\n",
      "         [[-0.1560,  0.6506, -1.5512,  ..., -0.8006,  0.2788, -0.4923],\n",
      "          [ 0.5500,  0.0803,  0.2996,  ..., -1.0741,  1.0641, -0.1096],\n",
      "          [-0.0794,  0.8292, -0.6421,  ..., -0.7845,  0.5959,  0.0406],\n",
      "          ...,\n",
      "          [ 0.1065,  0.7129,  1.1746,  ..., -0.6454,  0.5519,  0.1046],\n",
      "          [ 0.9074,  0.3966,  0.8883,  ..., -1.1463,  0.7440, -0.4715],\n",
      "          [ 0.4909,  0.1612,  0.5821,  ..., -0.8936,  0.2658, -0.0655]],\n",
      "\n",
      "         [[ 0.2471, -0.6504, -0.6385,  ..., -0.3214, -0.5552,  0.0372],\n",
      "          [-0.0699,  0.3589,  0.3867,  ...,  0.3255,  0.1814,  0.8557],\n",
      "          [ 1.4972, -0.1520, -0.1657,  ...,  0.4608,  1.1114,  0.7806],\n",
      "          ...,\n",
      "          [ 0.6815, -0.5538,  0.4626,  ...,  0.5160,  1.0786,  0.2739],\n",
      "          [ 1.1937, -0.7059,  0.0758,  ...,  0.9809,  0.8781,  0.4082],\n",
      "          [-0.0300, -0.0597,  0.3127,  ...,  0.6107, -0.0274,  1.0842]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "score_shape torch.Size([2, 8, 20, 20])\n",
      "tensor([[[[-0.3850, -0.3131, -0.1497,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3498, -1.0448,  0.2237,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.5132, -1.4104,  0.0521,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.9457, -0.5967, -0.3755,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4252, -0.3997,  0.2595,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.8607, -1.0611, -0.5475,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.1468, -0.3032, -0.1980,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7057, -0.0532, -0.4721,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.3531, -1.0253, -0.0693,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-1.3362,  0.0117,  0.0355,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5556,  0.1731,  0.9656,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.0577, -0.2503,  0.4202,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.2559, -0.6933, -0.3773,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7861, -0.7994, -0.1130,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.0924, -0.3091,  0.4341,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.3087, -0.1771, -0.0980,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3977,  0.0200, -0.0103,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7053, -0.1876, -0.2108,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5480,  0.3309,  0.9329,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5705,  0.0723,  1.2327,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.7102,  0.2708,  0.2389,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.6132,  0.4133,  0.1852,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.8715,  1.9727,  2.0733,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.0976,  0.4878,  0.7368,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6224,  2.2119,  1.8098,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0647,  1.5574,  0.8528,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2284,  1.4482,  0.1984,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.4875,  2.2162,  0.3504,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1732,  1.3051,  0.5165,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0294,  2.2375,  0.3412,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.7294, -0.3628, -2.4962,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5305,  0.5776,  0.6446,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.9246,  0.4018, -1.0387,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.5257,  0.0153, -1.9982,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.8278, -0.8950, -2.3001,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0731,  0.2250, -1.6016,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3566, -0.5505, -0.2973,  ..., -0.2068,  0.8781,  0.0151],\n",
      "          [ 1.0211, -1.0331,  0.0952,  ...,  0.1653,  0.3796,  0.0261],\n",
      "          [ 0.6850, -0.0172, -0.0701,  ..., -0.2668,  0.9105,  0.2062],\n",
      "          ...,\n",
      "          [-0.9187, -1.2526, -0.6446,  ...,  0.1246,  0.0120, -0.5275],\n",
      "          [-0.7675, -1.2269, -0.6085,  ..., -0.5994,  0.1920, -0.3876],\n",
      "          [ 0.0109, -0.8901, -0.0165,  ..., -0.3047,  0.2985, -0.0306]],\n",
      "\n",
      "         [[-0.4647, -0.4687, -0.1568,  ...,  0.0886, -0.5528, -0.2560],\n",
      "          [-0.4924, -0.8840, -0.4896,  ...,  0.1390, -1.0636, -0.2652],\n",
      "          [-0.6914, -0.4954, -0.5310,  ...,  0.2975, -0.8063,  0.1137],\n",
      "          ...,\n",
      "          [-1.7575, -0.7179, -0.8166,  ..., -0.2750, -1.2717, -1.1241],\n",
      "          [-0.5369,  0.5733,  0.2677,  ...,  1.2522, -0.9586,  0.7168],\n",
      "          [-0.6020, -0.3029, -0.9430,  ...,  0.5771, -0.8369, -0.0880]],\n",
      "\n",
      "         [[-1.7036,  0.3138, -0.5191,  ..., -0.6454, -1.4363, -1.2005],\n",
      "          [-1.3363, -0.0027, -0.0386,  ..., -1.9076, -0.7933, -1.3883],\n",
      "          [-1.2605, -0.1827, -0.9587,  ..., -0.2591, -1.0681, -1.5446],\n",
      "          ...,\n",
      "          [ 0.4887,  1.1163,  1.2836,  ...,  0.2158,  0.1709, -0.5715],\n",
      "          [-1.0790,  0.2151,  0.0106,  ..., -0.0497, -1.6229, -1.5140],\n",
      "          [-1.3052,  0.0537,  0.4044,  ..., -0.5633, -1.0491, -0.9597]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2807,  1.0597,  0.0157,  ...,  1.4587,  0.6821,  0.2875],\n",
      "          [ 1.0176,  0.9176,  0.4777,  ...,  0.9586,  1.6530,  0.8096],\n",
      "          [ 1.1942, -0.3718, -0.0485,  ...,  0.4467,  0.2861, -0.4185],\n",
      "          ...,\n",
      "          [ 0.6768, -0.0450,  0.1085,  ...,  0.6964,  0.5999, -0.0103],\n",
      "          [ 1.0244,  1.2453,  1.5747,  ...,  1.0817,  1.3683,  0.3244],\n",
      "          [ 0.3207,  0.6124,  0.6845,  ...,  1.3410,  1.8014,  0.6906]],\n",
      "\n",
      "         [[-0.3081,  1.0882,  1.5684,  ...,  0.2676,  1.9429,  0.4027],\n",
      "          [ 0.4101,  1.5052,  1.2975,  ...,  1.4493,  1.2370, -0.2330],\n",
      "          [ 0.0618,  0.2821,  0.2731,  ...,  0.9271,  0.3583, -0.7975],\n",
      "          ...,\n",
      "          [-0.0519,  1.4466,  1.1207,  ...,  0.9492,  1.6088,  0.1792],\n",
      "          [ 0.0225,  0.7750,  0.2509,  ...,  0.6929,  0.3443, -0.6650],\n",
      "          [ 0.4030,  1.4687,  1.1761,  ...,  0.4810,  0.3879, -0.3003]],\n",
      "\n",
      "         [[-0.3244, -0.3691, -1.3488,  ..., -1.6559, -1.4477, -0.5204],\n",
      "          [-0.1772, -0.5586, -0.3659,  ..., -0.8311, -0.5358, -0.2622],\n",
      "          [-0.2713, -0.6966, -2.0013,  ..., -2.0204, -1.5181, -0.7179],\n",
      "          ...,\n",
      "          [-0.3952,  0.0225, -1.0230,  ..., -1.7303, -1.3254, -0.8191],\n",
      "          [-1.6000, -1.5590, -2.1463,  ..., -2.5984, -2.1737, -1.2558],\n",
      "          [-0.6455, -0.5336, -1.2239,  ..., -1.6667, -1.6779, -0.5409]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[-0.3850, -0.3131, -0.1497,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3498, -1.0448,  0.2237,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.5132, -1.4104,  0.0521,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.9457, -0.5967, -0.3755,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4252, -0.3997,  0.2595,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.8607, -1.0611, -0.5475,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.1468, -0.3032, -0.1980,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7057, -0.0532, -0.4721,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.3531, -1.0253, -0.0693,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-1.3362,  0.0117,  0.0355,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5556,  0.1731,  0.9656,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.0577, -0.2503,  0.4202,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.2559, -0.6933, -0.3773,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7861, -0.7994, -0.1130,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.0924, -0.3091,  0.4341,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.3087, -0.1771, -0.0980,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3977,  0.0200, -0.0103,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7053, -0.1876, -0.2108,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5480,  0.3309,  0.9329,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5705,  0.0723,  1.2327,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.7102,  0.2708,  0.2389,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.6132,  0.4133,  0.1852,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.8715,  1.9727,  2.0733,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.0976,  0.4878,  0.7368,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6224,  2.2119,  1.8098,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0647,  1.5574,  0.8528,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2284,  1.4482,  0.1984,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.4875,  2.2162,  0.3504,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1732,  1.3051,  0.5165,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0294,  2.2375,  0.3412,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.7294, -0.3628, -2.4962,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5305,  0.5776,  0.6446,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.9246,  0.4018, -1.0387,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.5257,  0.0153, -1.9982,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.8278, -0.8950, -2.3001,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.0731,  0.2250, -1.6016,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3566, -0.5505, -0.2973,  ..., -0.2068,  0.8781,  0.0151],\n",
      "          [ 1.0211, -1.0331,  0.0952,  ...,  0.1653,  0.3796,  0.0261],\n",
      "          [ 0.6850, -0.0172, -0.0701,  ..., -0.2668,  0.9105,  0.2062],\n",
      "          ...,\n",
      "          [-0.9187, -1.2526, -0.6446,  ...,  0.1246,  0.0120, -0.5275],\n",
      "          [-0.7675, -1.2269, -0.6085,  ..., -0.5994,  0.1920, -0.3876],\n",
      "          [ 0.0109, -0.8901, -0.0165,  ..., -0.3047,  0.2985, -0.0306]],\n",
      "\n",
      "         [[-0.4647, -0.4687, -0.1568,  ...,  0.0886, -0.5528, -0.2560],\n",
      "          [-0.4924, -0.8840, -0.4896,  ...,  0.1390, -1.0636, -0.2652],\n",
      "          [-0.6914, -0.4954, -0.5310,  ...,  0.2975, -0.8063,  0.1137],\n",
      "          ...,\n",
      "          [-1.7575, -0.7179, -0.8166,  ..., -0.2750, -1.2717, -1.1241],\n",
      "          [-0.5369,  0.5733,  0.2677,  ...,  1.2522, -0.9586,  0.7168],\n",
      "          [-0.6020, -0.3029, -0.9430,  ...,  0.5771, -0.8369, -0.0880]],\n",
      "\n",
      "         [[-1.7036,  0.3138, -0.5191,  ..., -0.6454, -1.4363, -1.2005],\n",
      "          [-1.3363, -0.0027, -0.0386,  ..., -1.9076, -0.7933, -1.3883],\n",
      "          [-1.2605, -0.1827, -0.9587,  ..., -0.2591, -1.0681, -1.5446],\n",
      "          ...,\n",
      "          [ 0.4887,  1.1163,  1.2836,  ...,  0.2158,  0.1709, -0.5715],\n",
      "          [-1.0790,  0.2151,  0.0106,  ..., -0.0497, -1.6229, -1.5140],\n",
      "          [-1.3052,  0.0537,  0.4044,  ..., -0.5633, -1.0491, -0.9597]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2807,  1.0597,  0.0157,  ...,  1.4587,  0.6821,  0.2875],\n",
      "          [ 1.0176,  0.9176,  0.4777,  ...,  0.9586,  1.6530,  0.8096],\n",
      "          [ 1.1942, -0.3718, -0.0485,  ...,  0.4467,  0.2861, -0.4185],\n",
      "          ...,\n",
      "          [ 0.6768, -0.0450,  0.1085,  ...,  0.6964,  0.5999, -0.0103],\n",
      "          [ 1.0244,  1.2453,  1.5747,  ...,  1.0817,  1.3683,  0.3244],\n",
      "          [ 0.3207,  0.6124,  0.6845,  ...,  1.3410,  1.8014,  0.6906]],\n",
      "\n",
      "         [[-0.3081,  1.0882,  1.5684,  ...,  0.2676,  1.9429,  0.4027],\n",
      "          [ 0.4101,  1.5052,  1.2975,  ...,  1.4493,  1.2370, -0.2330],\n",
      "          [ 0.0618,  0.2821,  0.2731,  ...,  0.9271,  0.3583, -0.7975],\n",
      "          ...,\n",
      "          [-0.0519,  1.4466,  1.1207,  ...,  0.9492,  1.6088,  0.1792],\n",
      "          [ 0.0225,  0.7750,  0.2509,  ...,  0.6929,  0.3443, -0.6650],\n",
      "          [ 0.4030,  1.4687,  1.1761,  ...,  0.4810,  0.3879, -0.3003]],\n",
      "\n",
      "         [[-0.3244, -0.3691, -1.3488,  ..., -1.6559, -1.4477, -0.5204],\n",
      "          [-0.1772, -0.5586, -0.3659,  ..., -0.8311, -0.5358, -0.2622],\n",
      "          [-0.2713, -0.6966, -2.0013,  ..., -2.0204, -1.5181, -0.7179],\n",
      "          ...,\n",
      "          [-0.3952,  0.0225, -1.0230,  ..., -1.7303, -1.3254, -0.8191],\n",
      "          [-1.6000, -1.5590, -2.1463,  ..., -2.5984, -2.1737, -1.2558],\n",
      "          [-0.6455, -0.5336, -1.2239,  ..., -1.6667, -1.6779, -0.5409]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "score_shape torch.Size([2, 8, 20, 20])\n",
      "tensor([[[[-0.2544, -0.8069, -1.3713,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.5516, -1.9333, -1.1443,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.7422, -2.0913, -2.2453,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.8389, -1.3656, -1.3571,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.9247, -2.1914, -1.5793,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.9748, -1.1278, -0.7325,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1802,  0.5167,  0.4904,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.1565,  0.9401,  0.1163,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5084,  1.6810,  0.7076,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.0807,  0.5225,  0.1874,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.2011,  0.5550,  0.1505,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4187,  0.9480,  0.1792,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7591, -0.5729, -0.2395,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5343, -0.2632,  0.0894,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3643,  0.2297,  0.0619,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.1895,  0.0730,  0.5036,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3033, -0.1627,  0.5101,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.2366,  0.4039,  0.2297,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5021,  0.5278,  0.4701,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3783, -0.4160, -0.0747,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5826, -0.4031,  0.0982,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.0254, -0.4698, -0.4832,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7667, -0.9176, -0.6753,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3341, -0.6576, -0.1970,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4250, -0.6530, -1.0908,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5255, -0.9439, -1.2535,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2957, -0.4018, -0.5809,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.4590, -0.5447, -0.3950,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5050, -0.2023,  0.2780,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.9672, -1.0966, -1.4469,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5803, -0.0963, -0.5296,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3388, -1.3811, -1.5034,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.0728,  0.7683,  0.3416,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.2193, -0.6823, -0.0822,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1269, -0.9976, -0.3004,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4692,  0.0737, -0.3576,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.7111, -1.4779, -1.9876,  ..., -1.2919, -1.1705, -1.7132],\n",
      "          [-1.6790, -1.7014, -1.9131,  ..., -1.2466, -1.5106, -1.8450],\n",
      "          [-1.4441, -1.6462, -2.0518,  ..., -2.0132, -0.4020, -2.0072],\n",
      "          ...,\n",
      "          [-1.1790, -1.4283, -1.9353,  ..., -1.1239, -1.0368, -1.9315],\n",
      "          [-1.7365, -1.6096, -3.1271,  ..., -1.6881, -1.5156, -2.2470],\n",
      "          [-0.6398, -0.3669, -0.9880,  ..., -0.2866, -0.3146, -1.0152]],\n",
      "\n",
      "         [[-1.5889,  0.8589,  0.7384,  ...,  0.7333,  0.5940,  0.7864],\n",
      "          [ 0.3167,  2.0283,  1.9871,  ...,  1.3388,  2.2965,  1.9579],\n",
      "          [ 0.7951,  1.6226,  2.1728,  ...,  1.4678,  2.2861,  1.7340],\n",
      "          ...,\n",
      "          [-1.0030,  0.4865,  0.1896,  ...,  0.4155,  0.7618,  0.3345],\n",
      "          [-1.8099,  0.5009, -0.9623,  ...,  0.0038, -0.2041, -0.3228],\n",
      "          [-0.9625,  1.6548,  0.9217,  ...,  0.8407,  0.7561,  1.1737]],\n",
      "\n",
      "         [[-1.1280, -0.7486, -0.6317,  ..., -0.2913, -0.7425, -0.5170],\n",
      "          [-0.0347,  0.0244,  0.4099,  ...,  0.4695,  0.4033,  0.9735],\n",
      "          [-0.7184, -0.5521,  0.1006,  ..., -0.1200, -0.7823, -0.0128],\n",
      "          ...,\n",
      "          [-0.5382, -0.3878, -0.5852,  ...,  0.7744, -0.5176, -0.2203],\n",
      "          [-0.7564, -0.6665, -0.2049,  ...,  0.7082, -0.7955, -0.1116],\n",
      "          [-1.5309, -1.2888, -0.5635,  ..., -0.0893, -0.4365, -0.5777]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2450,  0.4444,  0.5247,  ...,  0.3196,  0.6941, -0.2244],\n",
      "          [-0.0918, -0.0950,  0.8002,  ..., -0.4497,  0.9156, -0.1143],\n",
      "          [ 0.6673,  0.0315,  0.7948,  ...,  0.5007,  0.8579, -0.4069],\n",
      "          ...,\n",
      "          [-0.6402, -0.3815, -0.1715,  ..., -0.4624,  0.4413, -0.6944],\n",
      "          [-0.3824, -0.4569,  0.3833,  ..., -0.3138,  0.1458, -0.3103],\n",
      "          [ 0.1215,  0.1498,  0.2890,  ..., -0.0236,  0.8036, -0.4479]],\n",
      "\n",
      "         [[-0.8125, -1.2940, -1.0740,  ..., -0.5912, -1.6951, -1.9993],\n",
      "          [-1.0294, -1.3537, -1.5159,  ..., -0.2630, -1.3878, -1.9761],\n",
      "          [ 0.9849,  1.3300,  0.6933,  ...,  1.5442,  0.7185,  0.7204],\n",
      "          ...,\n",
      "          [-0.0147,  0.0891, -0.8441,  ...,  0.5286, -0.1683, -0.9145],\n",
      "          [ 0.8478,  0.8547,  0.4768,  ...,  0.1345,  0.1523,  0.2601],\n",
      "          [-0.6106, -0.8018, -1.3174,  ..., -0.8268, -1.3127, -1.6344]],\n",
      "\n",
      "         [[ 0.0578, -0.5069,  0.2632,  ...,  0.5244, -0.1793, -0.7145],\n",
      "          [-0.3106, -1.5148, -1.2770,  ..., -0.3679, -1.4164, -1.0492],\n",
      "          [ 0.3544, -0.6825, -0.3212,  ...,  0.0202, -0.5247, -0.9608],\n",
      "          ...,\n",
      "          [-0.7691, -1.4234, -0.9119,  ..., -0.0913, -0.8083, -1.0994],\n",
      "          [ 0.6883,  0.0215, -0.0803,  ...,  1.4878,  0.1694,  0.4701],\n",
      "          [ 0.2613,  0.0652, -1.2406,  ..., -0.0679,  0.3165, -0.1555]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[-0.2544, -0.8069, -1.3713,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.5516, -1.9333, -1.1443,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.7422, -2.0913, -2.2453,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.8389, -1.3656, -1.3571,  ...,    -inf,    -inf,    -inf],\n",
      "          [-1.9247, -2.1914, -1.5793,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.9748, -1.1278, -0.7325,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1802,  0.5167,  0.4904,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.1565,  0.9401,  0.1163,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5084,  1.6810,  0.7076,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.0807,  0.5225,  0.1874,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.2011,  0.5550,  0.1505,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4187,  0.9480,  0.1792,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7591, -0.5729, -0.2395,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5343, -0.2632,  0.0894,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3643,  0.2297,  0.0619,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.1895,  0.0730,  0.5036,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3033, -0.1627,  0.5101,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.2366,  0.4039,  0.2297,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5021,  0.5278,  0.4701,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3783, -0.4160, -0.0747,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5826, -0.4031,  0.0982,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.0254, -0.4698, -0.4832,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.7667, -0.9176, -0.6753,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3341, -0.6576, -0.1970,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4250, -0.6530, -1.0908,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.5255, -0.9439, -1.2535,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.2957, -0.4018, -0.5809,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.4590, -0.5447, -0.3950,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.5050, -0.2023,  0.2780,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.9672, -1.0966, -1.4469,  ...,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5803, -0.0963, -0.5296,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3388, -1.3811, -1.5034,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 1.0728,  0.7683,  0.3416,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.2193, -0.6823, -0.0822,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1269, -0.9976, -0.3004,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4692,  0.0737, -0.3576,  ...,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.7111, -1.4779, -1.9876,  ..., -1.2919, -1.1705, -1.7132],\n",
      "          [-1.6790, -1.7014, -1.9131,  ..., -1.2466, -1.5106, -1.8450],\n",
      "          [-1.4441, -1.6462, -2.0518,  ..., -2.0132, -0.4020, -2.0072],\n",
      "          ...,\n",
      "          [-1.1790, -1.4283, -1.9353,  ..., -1.1239, -1.0368, -1.9315],\n",
      "          [-1.7365, -1.6096, -3.1271,  ..., -1.6881, -1.5156, -2.2470],\n",
      "          [-0.6398, -0.3669, -0.9880,  ..., -0.2866, -0.3146, -1.0152]],\n",
      "\n",
      "         [[-1.5889,  0.8589,  0.7384,  ...,  0.7333,  0.5940,  0.7864],\n",
      "          [ 0.3167,  2.0283,  1.9871,  ...,  1.3388,  2.2965,  1.9579],\n",
      "          [ 0.7951,  1.6226,  2.1728,  ...,  1.4678,  2.2861,  1.7340],\n",
      "          ...,\n",
      "          [-1.0030,  0.4865,  0.1896,  ...,  0.4155,  0.7618,  0.3345],\n",
      "          [-1.8099,  0.5009, -0.9623,  ...,  0.0038, -0.2041, -0.3228],\n",
      "          [-0.9625,  1.6548,  0.9217,  ...,  0.8407,  0.7561,  1.1737]],\n",
      "\n",
      "         [[-1.1280, -0.7486, -0.6317,  ..., -0.2913, -0.7425, -0.5170],\n",
      "          [-0.0347,  0.0244,  0.4099,  ...,  0.4695,  0.4033,  0.9735],\n",
      "          [-0.7184, -0.5521,  0.1006,  ..., -0.1200, -0.7823, -0.0128],\n",
      "          ...,\n",
      "          [-0.5382, -0.3878, -0.5852,  ...,  0.7744, -0.5176, -0.2203],\n",
      "          [-0.7564, -0.6665, -0.2049,  ...,  0.7082, -0.7955, -0.1116],\n",
      "          [-1.5309, -1.2888, -0.5635,  ..., -0.0893, -0.4365, -0.5777]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2450,  0.4444,  0.5247,  ...,  0.3196,  0.6941, -0.2244],\n",
      "          [-0.0918, -0.0950,  0.8002,  ..., -0.4497,  0.9156, -0.1143],\n",
      "          [ 0.6673,  0.0315,  0.7948,  ...,  0.5007,  0.8579, -0.4069],\n",
      "          ...,\n",
      "          [-0.6402, -0.3815, -0.1715,  ..., -0.4624,  0.4413, -0.6944],\n",
      "          [-0.3824, -0.4569,  0.3833,  ..., -0.3138,  0.1458, -0.3103],\n",
      "          [ 0.1215,  0.1498,  0.2890,  ..., -0.0236,  0.8036, -0.4479]],\n",
      "\n",
      "         [[-0.8125, -1.2940, -1.0740,  ..., -0.5912, -1.6951, -1.9993],\n",
      "          [-1.0294, -1.3537, -1.5159,  ..., -0.2630, -1.3878, -1.9761],\n",
      "          [ 0.9849,  1.3300,  0.6933,  ...,  1.5442,  0.7185,  0.7204],\n",
      "          ...,\n",
      "          [-0.0147,  0.0891, -0.8441,  ...,  0.5286, -0.1683, -0.9145],\n",
      "          [ 0.8478,  0.8547,  0.4768,  ...,  0.1345,  0.1523,  0.2601],\n",
      "          [-0.6106, -0.8018, -1.3174,  ..., -0.8268, -1.3127, -1.6344]],\n",
      "\n",
      "         [[ 0.0578, -0.5069,  0.2632,  ...,  0.5244, -0.1793, -0.7145],\n",
      "          [-0.3106, -1.5148, -1.2770,  ..., -0.3679, -1.4164, -1.0492],\n",
      "          [ 0.3544, -0.6825, -0.3212,  ...,  0.0202, -0.5247, -0.9608],\n",
      "          ...,\n",
      "          [-0.7691, -1.4234, -0.9119,  ..., -0.0913, -0.8083, -1.0994],\n",
      "          [ 0.6883,  0.0215, -0.0803,  ...,  1.4878,  0.1694,  0.4701],\n",
      "          [ 0.2613,  0.0652, -1.2406,  ..., -0.0679,  0.3165, -0.1555]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "score_shape torch.Size([2, 8, 20, 20])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (19) must match the size of tensor b (20) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-026905fa7568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-b941c20f4024>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_padding_mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msrc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtgt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-3d0574a8cba3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moutput\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-3d0574a8cba3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-3d0574a8cba3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feedforward_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_attn_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multihead_attn_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feedforward_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-3d0574a8cba3>\u001b[0m in \u001b[0;36m_self_attn_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_self_attn_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-3d0574a8cba3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mattn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mattn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-3d0574a8cba3>\u001b[0m in \u001b[0;36m_attention\u001b[0;34m(self, query, key, value, key_padding_mask, attn_mask)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score_shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_padding_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (19) must match the size of tensor b (20) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "model = TransformerWrapper(emb_size=512, src_vocab_size=len(en_vocab), tgt_vocab_size=len(de_vocab))\n",
    "for src, tgt in train_loader:\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = generate_mask(src, tgt_input)\n",
    "    print(src_mask.shape)\n",
    "    print(src_padding_mask)\n",
    "    print(src_padding_mask.shape)\n",
    "    print(tgt_padding_mask.shape)\n",
    "    logits = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, memory_padding_mask=src_padding_mask)\n",
    "    print(logits)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0789,  0.6511,  0.3624,  1.0269, -1.7361],\n",
      "        [ 0.6671,  1.7341,  0.1811,  0.4002,  0.8695],\n",
      "        [-0.6923, -0.9453,  1.4208,  0.9455,  0.0064],\n",
      "        [-0.1542, -1.2689, -1.1903,  0.4219, -0.6572],\n",
      "        [-1.2416, -1.8163, -2.2015, -1.5558, -0.2969]])\n",
      "tensor([[-0.0789,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.6671,  1.7341,    -inf,    -inf,    -inf],\n",
      "        [-0.6923, -0.9453,  1.4208,    -inf,    -inf],\n",
      "        [-0.1542, -1.2689, -1.1903,  0.4219,    -inf],\n",
      "        [-1.2416, -1.8163, -2.2015, -1.5558, -0.2969]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((5, 5))\n",
    "print(a)\n",
    "mask = generate_mask(a, a)\n",
    "tgt_mask = mask[1]\n",
    "a = a.masked_fill(tgt_mask == 0, float('-inf'))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=1)\n",
    "model = TransformerWrapper(emb_size=512, src_vocab_size=len(en_vocab), tgt_vocab_size=len(de_vocab))\n",
    "pl_model = TransformerTrainer(model=model)\n",
    "trainer.fit(model=pl_model, train_dataloaders=train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d54d44b476ee03c59f2b2873b2d13a0a58dbc0ce917a1197d2792a3bd70a14c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
