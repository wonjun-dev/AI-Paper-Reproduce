{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer and build vocabulary\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_md')\n",
    "\n",
    "def yield_tokens(data_iter, language: str):\n",
    "    for text in data_iter:\n",
    "        if language == 'en':\n",
    "            yield en_tokenizer(text[0])\n",
    "        elif language == 'de':\n",
    "            yield de_tokenizer(text[1])\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'en'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "de_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'de'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "en_vocab.set_default_index(UNK_IDX) # oov 일때 반환하는 토큰\n",
    "de_vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166, 37, 8, 336, 288, 18, 1225, 4, 759, 4497, 2958, 6]\n",
      "[85, 32, 11, 848, 2209, 16, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "# Set preprocess pipeline\n",
    "en_pipeline = lambda x: en_vocab(en_tokenizer(x))\n",
    "de_pipeline = lambda x: de_vocab(de_tokenizer(x))\n",
    "trasnform_pipeline = {'en': en_pipeline, 'de': de_pipeline}\n",
    "print(en_pipeline('Several men in hard hats are operating a giant pulley system.'))\n",
    "print(de_pipeline('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "\n",
    "\n",
    "def collate_func(batch, src_ln: str = 'de', tgt_ln: str = 'en', batch_first: bool = True):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src, tgt in batch:\n",
    "        src_ids = trasnform_pipeline[src_ln](src.rstrip('\\n'))\n",
    "        tgt_ids = trasnform_pipeline[tgt_ln](tgt.rstrip('\\n'))\n",
    "        src_ids = torch.cat((torch.tensor([BOS_IDX]), torch.tensor(src_ids), torch.tensor([EOS_IDX])))\n",
    "        tgt_ids = torch.cat((torch.tensor([BOS_IDX]), torch.tensor(tgt_ids), torch.tensor([EOS_IDX])))\n",
    "        src_batch.append(src_ids)\n",
    "        tgt_batch.append(tgt_ids)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=batch_first)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=batch_first)\n",
    "    \n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = Multi30k(root='./data', split=('train', 'valid', 'test'), language_pair=('de', 'en'))\n",
    "train_loader = DataLoader(list(train_iter), batch_size=512, collate_fn=collate_func, num_workers=8, shuffle=True)\n",
    "valid_loader = DataLoader(list(valid_iter), batch_size=1, collate_fn=collate_func)\n",
    "test_loader = DataLoader(list(test_iter), batch_size=1, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer\n",
    "# I follow style of official Pytorch Transformer source code and adjust it simply\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.modules import LayerNorm\n",
    "from typing import Union, Callable, Optional, Any, Tuple\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_head: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, \\\n",
    "        dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \\\n",
    "        layer_norm_eps: float = 1e-5, batch_first: bool = True, norm_first: bool = False, device=None, dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Transformer, self).__init__()\n",
    "   \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, n_head, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)\n",
    "        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_head, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)\n",
    "        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_params()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, \\\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output  = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "    \n",
    "    def _reset_params(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dim_feedforward: int = 2048, dropout: float = 0.1, \\\n",
    "                activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,  \n",
    "                device=None, dtype=None) -> None:\n",
    "\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Define params\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, dropout, batch_first, **factory_kwargs)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.droput = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._self_attn_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            x = x + self._feedforward_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._self_attn_block(x, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._feedforward_block(x))\n",
    "        return x\n",
    "\n",
    "    def _self_attn_block(self, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    def _feedforward_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.droput(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dim_feedforward: int = 2048, droput: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \\\n",
    "                layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        # Define params\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, droput, batch_first, **factory_kwargs)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, n_head, droput, batch_first, **factory_kwargs)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.droput = nn.Dropout(droput)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(droput)\n",
    "        self.dropout2 = nn.Dropout(droput)\n",
    "        self.dropout3 = nn.Dropout(droput)\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._self_attn_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._multihead_attn_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
    "            x = x + self._feedforward_block(self.norm3(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._self_attn_block(x, tgt_mask ,tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._multihead_attn_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "            x = self.norm3(x + self._feedforward_block(x))\n",
    "        return x\n",
    "    \n",
    "    def _self_attn_block(self, x: Tensor, attn_mask: Optional[Tensor] = None, key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _multihead_attn_block(self, x: Tensor, mem: Tensor, attn_mask: Optional[Tensor] = None, key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = self.multihead_attn(x, mem, mem, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout2(x)\n",
    "    \n",
    "    def _feedforward_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.droput(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, batch_first=False, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = int(embed_dim // num_heads)\n",
    "        assert self.embed_dim == self.head_dim * num_heads\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, **factory_kwargs)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, **factory_kwargs)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, **factory_kwargs)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, need_weights: bool = True, \\\n",
    "                attn_mask: Optional[Tensor] = None, average_attn_weights: bool = True) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        query, key, value = self.wq(query), self.wk(key), self.wv(value)\n",
    "        query, key ,value = self._split_heads(query), self._split_heads(key), self._split_heads(value)\n",
    "        attn_out = self._attention(query, key, value, key_padding_mask, attn_mask)\n",
    "        attn_out = self.out_proj(attn_out)\n",
    "        return attn_out\n",
    "    \n",
    "    def _split_heads(self, proj: Tensor) -> Tensor:\n",
    "        if self.batch_first:    # (N, L, E)\n",
    "            bs = proj.size(0)\n",
    "            proj = proj.view(bs, -1, self.num_heads, self.head_dim)    # (N, L, H, E_Hi)\n",
    "            proj = proj.transpose(1, 2) # (N, H, L, E_Hi)\n",
    "        else:   # (L, N, E)\n",
    "            bs = proj.size(1)\n",
    "            proj = proj.view(-1, bs, self.num_heads, self.head_dim)    # (L, N, H, E_Hi)\n",
    "        return proj\n",
    "    \n",
    "    def _attention(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, attn_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        if self.batch_first:\n",
    "            score = torch.matmul(query, key.transpose(-1, -2))  # (N, H, QL, KL), score = Q * K^T\n",
    "            score = score / math.sqrt(query.size(-1)) # score = Q * K^T divided by sqrt(E_Hi)\n",
    "            if key_padding_mask is not None:\n",
    "                score = score.masked_fill(key_padding_mask == 0, float('-inf'))\n",
    "            if attn_mask is not None:\n",
    "                score = score.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "            softmax = F.softmax(score, dim=-1)\n",
    "            attn_out = torch.matmul(self.dropout(softmax), value)   # (N, H, QL, E_Hi)\n",
    "            attn_out = attn_out.transpose(1, 2) # (N, QL, H, E_Hi)\n",
    "            attn_out = attn_out.contiguous().view(attn_out.size(0), -1, self.embed_dim)  # (N, QL, E)\n",
    "            return attn_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int = 512, dropout: float = 0.1, max_len: int = 5000, device=None) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1)\n",
    "        val = torch.exp(-torch.arange(0, emb_size, 2) / emb_size * math.log(10000))\n",
    "        pos_encoding = torch.zeros((max_len, emb_size))\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos * val)\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos * val)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).to(device)    # batch first\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "    \n",
    "    def forward(self, token_embedding: Tensor) -> Tensor:\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:, :token_embedding.size(1), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int = 512, device=None) -> None:\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, device=device)\n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def forward(self, tokens: Tensor) -> Tensor:\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWrapper(nn.Module):\n",
    "    def __init__(self, emb_size: int, src_vocab_size: int, tgt_vocab_size: int, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size, num_encoder_layers=3, num_decoder_layers=3, n_head=8, dim_feedforward=1024, batch_first=True, device=device)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size, device=device)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size, device=device)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size, device=device)\n",
    "        self.pos_encoding = PositionalEncoding(emb_size=emb_size, device=device)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, src_padding_mask: Optional[Tensor] = None, tgt_padding_mask: Optional[Tensor] = None, memory_padding_mask: Optional[Tensor] = None):\n",
    "        src_emb = self.dropout1(self.pos_encoding(self.src_tok_emb(src)))\n",
    "        tgt_emb = self.dropout2(self.pos_encoding(self.tgt_tok_emb(tgt)))\n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask, src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_padding_mask)\n",
    "        return self.generator(output)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        src_emb = self.pos_encoding(self.src_tok_emb(src))\n",
    "        return self.transformer.encoder(src_emb, src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor) -> Tensor:\n",
    "        tgt_emb = self.pos_encoding(self.tgt_tok_emb(tgt))\n",
    "        return self.transformer.decoder(tgt_emb, memory, tgt_mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src: Tensor, tgt: Tensor) -> Tuple[Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [N, L]\n",
    "            tgt: [N, L]\n",
    "        \"\"\"\n",
    "        src_seq_len = src.shape[-1] # L\n",
    "        tgt_seq_len = tgt.shape[-1]\n",
    "\n",
    "        \"\"\"\n",
    "        tgt mask\n",
    "        1 0 0 0 0 \n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 1 0\n",
    "        1 1 1 1 1\n",
    "        \"\"\"\n",
    "        tgt_mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1).float()\n",
    "        src_mask = torch.ones((src_seq_len, src_seq_len)).type(torch.bool)\n",
    "        src_padding_mask = (src != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "        tgt_padding_mask = (tgt != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7, 13, 13,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         3,  4,  6,  4,  4,  4,  6,  6,  4,  4,  4,  6,  4,  6,  4,  4,  6,  6,\n",
      "         6], device='cuda:0') tensor([   7,  232,  138,   15,  110,    8,   27,  235,  255,   19, 2257,   30,\n",
      "         345,   49,  222,    8,   25,    6,    3,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1], device='cuda:0')\n",
      "Epoch 0 => Train Loss 5.539744418964052 Train PPL 254.61291690890678\n",
      "Epoch 0 => Vaid Loss 4.893062008438261 Valid PPL 133.3613029329944\n",
      "tensor([ 7, 13, 13,  8,  4,  4,  8,  4,  4,  4,  4,  6,  6,  3,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([   7,  159,   11,  256,    4, 1344,   10,    9,  497,   14,    4, 2327,\n",
      "           6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 1 => Train Loss 5.092504066333436 Train PPL 162.79700658566182\n",
      "Epoch 1 => Vaid Loss 4.870747577744358 Valid PPL 130.41837833901857\n",
      "tensor([ 7, 13,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  3,  4,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([  7,  40,  14,  53,   8, 305, 288, 105,  21,   4, 100,   6,   3,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 2 => Train Loss 5.002100216714959 Train PPL 148.72518644787309\n",
      "Epoch 2 => Vaid Loss 4.783710668072898 Valid PPL 119.54712774635608\n",
      "tensor([ 7, 13,  8,  6,  4,  4,  6,  6,  6,  4,  6,  6,  6,  3,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([  20, 1047,  118,   18, 2176,    4,   25, 4159,   10,    4,  254, 1309,\n",
      "           6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 3 => Train Loss 4.903371417731569 Train PPL 134.7432906872835\n",
      "Epoch 3 => Vaid Loss 4.733167110580429 Valid PPL 113.65495073386221\n",
      "tensor([ 7, 13,  8,  8,  8,  8,  8,  4,  4,  4,  4,  8,  3,  4,  6,  6,  6,  6,\n",
      "         4,  6,  6,  4,  6,  6,  4,  6,  6,  3,  6,  6,  6], device='cuda:0') tensor([   7, 1689, 1834,   26,   13,   16,   15,  172,   27,  103,   12,    4,\n",
      "          52,  452,   16,  734,  187,    8,    4,  248,   47,    4,  539,   19,\n",
      "          28, 1245,    6,    3,    1,    1,    1], device='cuda:0')\n",
      "Epoch 4 => Train Loss 4.751998374336644 Train PPL 115.81549614183936\n",
      "Epoch 4 => Vaid Loss 4.585784697438603 Valid PPL 98.08012017266246\n",
      "tensor([7, 8, 8, 4, 8, 6, 6, 6, 6, 6, 4, 6, 4, 6, 4, 0, 6, 6, 3, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([  84,  579,   56,    4, 2569,  293,  191,   16,  339,   56, 1739,   12,\n",
      "         220,   10,    4,  411,  191,    6,    3,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 5 => Train Loss 4.663781375215764 Train PPL 106.03628800698718\n",
      "Epoch 5 => Vaid Loss 4.390397578066388 Valid PPL 80.67248621652041\n",
      "tensor([ 7, 13,  8,  8,  8,  8,  8,  8,  4,  8,  6,  3,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6, 13,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([  7,  40,  14, 521,  18, 261, 321,  19, 625, 789,   6,   3,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 6 => Train Loss 4.521009495383815 Train PPL 91.928352400089\n",
      "Epoch 6 => Vaid Loss 4.330471030001104 Valid PPL 75.9800670327482\n",
      "tensor([ 7, 13,  8,  4,  4,  0,  4,  4,  6,  3,  4,  6,  6,  6,  6,  6,  4,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6], device='cuda:0') tensor([  54,  246,   73,    8,    4,   91,   14, 1186,    6,    3,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 7 => Train Loss 4.472321409928171 Train PPL 87.55974934002725\n",
      "Epoch 7 => Vaid Loss 4.3465242785579825 Valid PPL 77.20963682333284\n",
      "tensor([7, 8, 8, 4, 0, 6, 6, 4, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([ 251,  129,   22,    4,   69,   33,    8,   45,   14,   46,  362,  796,\n",
      "        2094,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 8 => Train Loss 4.395501061489708 Train PPL 81.08524928148823\n",
      "Epoch 8 => Vaid Loss 4.258842895134431 Valid PPL 70.72809626306096\n",
      "tensor([ 7, 13,  8,  8,  8,  8,  8,  4,  4, 41,  6,  3,  6,  6,  6,  3,  6,  6,\n",
      "         6,  3,  3,  6,  6,  6,  3,  6,  6,  6,  6,  6,  6,  6,  3,  6,  6],\n",
      "       device='cuda:0') tensor([   7,  185, 2619,  165,    0,  165, 1917,   10,    9,   48,    6,    3,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 9 => Train Loss 4.306166841272722 Train PPL 74.15569291340043\n",
      "Epoch 9 => Vaid Loss 4.12786544227506 Valid PPL 62.045342120080996\n",
      "tensor([ 7, 13, 14,  4,  8, 14,  8, 14, 12,  6,  4,  4,  4,  6,  4,  6,  3,  6,\n",
      "         4,  6,  4,  6,  4,  4,  4,  4,  4,  4], device='cuda:0') tensor([   7,   40,   14,  245,    0,    0, 1249,   16, 1279,   19, 2882,   71,\n",
      "         729,   72, 2461,    6,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 10 => Train Loss 4.261550610525566 Train PPL 70.91986733146719\n",
      "Epoch 10 => Vaid Loss 4.157725218249966 Valid PPL 63.92593951727739\n",
      "tensor([ 7, 13, 13, 14, 23,  8,  6, 33, 14,  4, 13,  6,  6,  4,  6,  3,  6,  6,\n",
      "         6,  6,  6,  6,  4,  6,  6,  6,  4,  3,  4,  6,  6], device='cuda:0') tensor([   7,   63,   40,   14,  645, 1249,   18, 2344,   42,    4,  148,  487,\n",
      "          15,  253,    6,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 11 => Train Loss 4.211952460439582 Train PPL 67.48817925803154\n",
      "Epoch 11 => Vaid Loss 4.101422878294537 Valid PPL 60.42620559380082\n",
      "tensor([ 7, 13,  8,  4,  0,  4,  4,  6,  6,  4,  0,  0,  6,  4,  6,  3,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6], device='cuda:0') tensor([   7,   13,    8,    4, 1302,   12,   69, 4647,   19,    4,   63,    0,\n",
      "          14,  396,    6,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1], device='cuda:0')\n",
      "Epoch 12 => Train Loss 4.146836966799016 Train PPL 63.233673436385324\n",
      "Epoch 12 => Vaid Loss 4.003733965889707 Valid PPL 54.8023987545771\n",
      "tensor([ 7, 13,  8, 39,  8,  4, 31, 14,  4, 31,  6,  3,  6,  6,  6,  6,  6, 24,\n",
      "        24,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([  7,  73,  11, 413,  21,   4, 100,   8,   4, 849,   6,   3,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 13 => Train Loss 4.103607654571533 Train PPL 60.55836765398778\n",
      "Epoch 13 => Vaid Loss 3.9897756391963544 Valid PPL 54.04276292530761\n",
      "tensor([ 7, 37, 18,  8,  4, 12,  6,  3,  6,  6,  6,  6,  4,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([ 20, 118,  43, 125, 180, 298,   6,   3,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 14 => Train Loss 4.0272152340203 Train PPL 56.10445572261926\n",
      "Epoch 14 => Vaid Loss 3.9009627113210614 Valid PPL 49.45003230333878\n",
      "tensor([ 7, 13, 14, 23, 18,  6,  4,  0,  6,  3,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6], device='cuda:0') tensor([  7,  40,  14, 433, 137,  56,   4, 140,   6,   3,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "Epoch 15 => Train Loss 3.9755720464806807 Train PPL 53.280587162077914\n",
      "Epoch 15 => Vaid Loss 3.8917837657872036 Valid PPL 48.99820994884984\n",
      "tensor([ 7, 37, 34, 18, 43,  4,  4, 41,  6,  4, 25,  8,  6,  3, 12, 24,  6, 24,\n",
      "         6,  6,  6,  6, 24,  6,  6,  6,  6, 24,  6,  6,  6, 24, 24, 24,  6,  6,\n",
      "         6,  6,  6,  6, 24], device='cuda:0') tensor([ 20,  26,  74,  18,  39,  10,   9,  93,  30,   4,  13, 231,   6,   3,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "Epoch 16 => Train Loss 3.9367933607937995 Train PPL 51.25398447211315\n",
      "Epoch 16 => Vaid Loss 3.8055954840056287 Valid PPL 44.95201034775972\n",
      "tensor([ 7, 13, 35,  4, 37,  6,  6,  4,  6,  6,  3,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6], device='cuda:0') tensor([251, 194,  12,  67, 181, 751,  47, 786, 437,   6,   3,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1], device='cuda:0')\n",
      "Epoch 17 => Train Loss 3.8870963882981684 Train PPL 48.76907428474735\n",
      "Epoch 17 => Vaid Loss 3.782292460783934 Valid PPL 43.91660351588803\n",
      "tensor([ 7, 37, 18,  4,  8, 11, 38,  6,  4,  9, 41,  6,  4,  0,  6,  6,  3,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([  20,   23,    8,  570,  360,   18, 1825,  716,    8,    9,  102,  234,\n",
      "           4, 2908,  191,    6,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 18 => Train Loss 3.845718446530794 Train PPL 46.79229003596699\n",
      "Epoch 18 => Vaid Loss 3.741983739229349 Valid PPL 42.181584486818224\n",
      "tensor([ 7, 13,  8,  4,  0, 24, 11,  4,  6, 11,  4,  0,  6,  6,  6,  3,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6], device='cuda:0') tensor([  7,  13,   8,   4,  31,  24,  12, 196, 290, 145,   4,  78, 371,  36,\n",
      "          6,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1], device='cuda:0')\n",
      "Epoch 19 => Train Loss 3.8248256047566733 Train PPL 45.82480804089326\n",
      "Epoch 19 => Vaid Loss 3.7407914697536113 Valid PPL 42.13132263998318\n",
      "tensor([ 7, 13,  8,  8,  4,  4,  6,  4,  6,  3,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6], device='cuda:0') tensor([   7,   17,   77,    8, 3628, 2276,   19,    0,    6,    3,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 20 => Train Loss 3.7862754788315085 Train PPL 44.091872959245556\n",
      "Epoch 20 => Vaid Loss 3.677520062561336 Valid PPL 39.548195309326296\n",
      "tensor([ 7, 13,  8,  4, 32, 24, 14,  4, 25,  6,  3,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([  7,  17,   8,   4,  52, 114,  82,   4, 106,   6,   3,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 21 => Train Loss 3.742879407447681 Train PPL 42.2193821159827\n",
      "Epoch 21 => Vaid Loss 3.6577828373664465 Valid PPL 38.77527640096668\n",
      "tensor([  7,  13,   8,   4,  31, 191,   6,   3,   6,   6,   6,   3,   0,   6,\n",
      "          6,   6,   6,   6,   3,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   3,   0], device='cuda:0') tensor([  7,  17, 233,   4, 558, 495,   6,   3,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 22 => Train Loss 3.7165443018863074 Train PPL 41.122042922750566\n",
      "Epoch 22 => Vaid Loss 3.6449661733365857 Valid PPL 38.28147789498831\n",
      "tensor([ 7,  8,  8, 38,  4, 41,  6,  4,  0, 12,  6,  3,  6,  6,  6,  6,  6,  6,\n",
      "         3,  6,  6,  6,  6,  6,  6,  6,  3,  6,  6,  6,  3,  6,  6],\n",
      "       device='cuda:0') tensor([279,  73,  11, 587,   9,  81, 200,   4,  52, 576,   6,   3,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 23 => Train Loss 3.6962178590004906 Train PPL 40.2946158786226\n",
      "Epoch 23 => Vaid Loss 3.7215968418638616 Valid PPL 41.330339458745364\n",
      "tensor([  7, 124,  17,   8,   8,   9,   0,  41,   6,   6,  19,   4,   6,   3,\n",
      "          6,   6,   6,   6,   6,   3,   6,   6,   6,   6,   6,   6,   6,   6],\n",
      "       device='cuda:0') tensor([ 54, 142,  17,  38,   8,   4, 351, 736, 179, 591,  19, 342,   6,   3,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "Epoch 24 => Train Loss 3.6855036710437976 Train PPL 39.865196339169984\n",
      "Epoch 24 => Vaid Loss 3.6325194994373433 Valid PPL 37.807953836187636\n",
      "tensor([  7,  18,   4,  92,   0,  11, 103,   6,   3,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6],\n",
      "       device='cuda:0') tensor([  84,   21,   29,  257, 1914,  107,  293,    6,    3,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 25 => Train Loss 3.6519103552165784 Train PPL 38.548236578573075\n",
      "Epoch 25 => Vaid Loss 3.596640447187706 Valid PPL 36.47548704586049\n",
      "tensor([  7,  53,  18,  33,   4,   6,   4,   0,  65,   6,   6,   6,   3,  11,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "        165,   6], device='cuda:0') tensor([  20, 2118,   18,  193,    0,    8,    4,   78, 1282,  264, 1710,    6,\n",
      "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 26 => Train Loss 3.6249445112128007 Train PPL 37.522641015991226\n",
      "Epoch 26 => Vaid Loss 3.579357735268933 Valid PPL 35.850507934403694\n",
      "tensor([ 7, 13,  8, 33,  4, 31, 24,  6,  6,  6,  4,  6,  3,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([   7,   13,   11,  511,    4,   25,  828, 2114,  534,   15,  403,    6,\n",
      "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 27 => Train Loss 3.5945509525767543 Train PPL 36.39935128258052\n",
      "Epoch 27 => Vaid Loss 3.557247499037071 Valid PPL 35.06654349176476\n",
      "tensor([  7,  13,  40,   8,  10,   4,   0,  10,  11,   0,   6,   4, 104,  14,\n",
      "          3,  12,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6], device='cuda:0') tensor([   7,   63,  283,  531,   85,    4, 1299,  117,  726,    0,   10,    9,\n",
      "          45,    6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 28 => Train Loss 3.5891357890346596 Train PPL 36.20277556774653\n",
      "Epoch 28 => Vaid Loss 3.561318684494237 Valid PPL 35.209596894540894\n",
      "tensor([  7,  13,   8,   4,  24,  24,  11,  21,   4, 143,   6,   3,   6,   6,\n",
      "          6,   6,   6,   3,   6,  19,   6,   3,   6,   6,   6,   6,  19,  19,\n",
      "          3,   6,  19,  19,   6,   0,   6,  19,   6,   6,   6],\n",
      "       device='cuda:0') tensor([  7,  17,  15,  32,  44, 103,  57,  19,  46, 475,   6,   3,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "Epoch 29 => Train Loss 3.557302893253795 Train PPL 35.068486029276876\n",
      "Epoch 29 => Vaid Loss 3.547931210999423 Valid PPL 34.741370526511346\n",
      "tensor([ 7, 13,  8,  4, 41,  6,  6,  4,  0,  6,  6,  4,  0,  8,  4, 41,  6,  4,\n",
      "         0,  6,  3, 11,  6,  6,  6,  6,  6,  6,  6,  6,  6,  0,  6,  6,  6,  6,\n",
      "         6,  6], device='cuda:0') tensor([   7,   13,    8,    9,  744, 2567,   10,    4, 2704,  394,   12,    4,\n",
      "          17,    8,    9,  104,   10,    4, 2690,    6,    3,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "Epoch 30 => Train Loss 3.542499492042943 Train PPL 34.55317673623631\n",
      "Epoch 30 => Vaid Loss 3.5422371406056707 Valid PPL 34.54411284967105\n",
      "tensor([ 7, 11,  4,  8, 10,  4,  6,  4,  4,  4,  0,  6,  3,  6,  6,  0,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6, 41,  6,  6],\n",
      "       device='cuda:0') tensor([224,  11,  73,  38,   8,  48,  12,  39,  15,   4,  35,   6,   3,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 31 => Train Loss 3.530355930328369 Train PPL 34.1361155315305\n",
      "Epoch 31 => Vaid Loss 3.545383918332395 Valid PPL 34.65298670564739\n",
      "tensor([ 7, 13, 14, 23, 18, 21,  4, 76, 18,  4,  6,  3,  6,  4,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6], device='cuda:0') tensor([   7,   94,   14,   23,   57,   21,   67,  389,   10, 1113,    6,    3,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 32 => Train Loss 3.513781271482769 Train PPL 33.5749841886454\n",
      "Epoch 32 => Vaid Loss 3.5326593165566935 Valid PPL 34.21483481558738\n",
      "tensor([  7,  37,   8,  10,   4, 163,  41,  14,   0,   6,   4,   6,   4,  24,\n",
      "          6,   3,   6,   0,   6,   6,   6,   6,   6,   6,   6,   6,   0,   6,\n",
      "          6], device='cuda:0') tensor([  20,   53, 1925,    8, 1137,    9, 3724,  264, 1066,   14,  123,    8,\n",
      "          65,  483,    6,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 33 => Train Loss 3.499478946652329 Train PPL 33.09820153617508\n",
      "Epoch 33 => Vaid Loss 3.5400592742351855 Valid PPL 34.46896225168146\n",
      "tensor([  7,  37,   8,   4, 464,  42,  14,   4, 784,   8,   3,  11,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6], device='cuda:0') tensor([ 20,  37,  47,  71, 372,  79,  68,  71,   0,   6,   3,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1], device='cuda:0')\n",
      "Epoch 34 => Train Loss 3.4837938317081383 Train PPL 32.58310269337564\n",
      "Epoch 34 => Vaid Loss 3.5192567937707997 Valid PPL 33.7593289943616\n",
      "tensor([ 7,  8,  4, 32,  8,  4, 31,  6, 33,  8,  4,  0,  6,  6,  3,  6,  0,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6], device='cuda:0') tensor([ 900,    8,    4,  292,   21,    4,  239,   18, 1998,   64,    4, 1473,\n",
      "         243,    6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1], device='cuda:0')\n",
      "Epoch 35 => Train Loss 3.4726874828338623 Train PPL 32.223225547283946\n",
      "Epoch 35 => Vaid Loss 3.5343806711879706 Valid PPL 34.273781399387886\n",
      "tensor([  7,  13, 491,  11,   4,  24,   4,  24,  11,   3,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6], device='cuda:0') tensor([  50, 1304,  491,   22,   31,   12,   27,  152,    6,    3,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 36 => Train Loss 3.474341911182069 Train PPL 32.27658068905955\n",
      "Epoch 36 => Vaid Loss 3.5133106076035507 Valid PPL 33.559185374599686\n",
      "tensor([  7,  13,   0,   4,  32,   8,  38,  19,   4,   4,   3,  11,   6,   6,\n",
      "        190,   0,   6,   0, 190,   0,   6, 190,   6,   6, 190, 190,   6,   6,\n",
      "          6,   6], device='cuda:0') tensor([  7,   0,  12,   4, 864,  18, 321,  19, 515,   6,   3,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1], device='cuda:0')\n",
      "Epoch 37 => Train Loss 3.456676959991455 Train PPL 31.711422905324092\n",
      "Epoch 37 => Vaid Loss 3.5092522425529284 Valid PPL 33.42326594143975\n",
      "tensor([  7,  13,   8,   4,  27,  24,  42,   4,   0, 518,   4,   6,   3,   6,\n",
      "          6,   6,   6,   6,   6, 190, 190,   6, 190,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6,   6], device='cuda:0') tensor([   7,   17,    8,  262,   27,  333,   72,    4,  370,  234, 1529,    6,\n",
      "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 38 => Train Loss 3.4260601913719846 Train PPL 30.755234006817133\n",
      "Epoch 38 => Vaid Loss 3.505687047626375 Valid PPL 33.30431764619797\n",
      "tensor([  7,  13,  36,  18,  10,   4,  14,   4, 177,   6,   4,  19,   4,   6,\n",
      "         41,   6,   6,   4,   0,   6,   4, 104,   6,   4,   0,   6, 143,   6,\n",
      "         19,   4,   6,   6,   3], device='cuda:0') tensor([   7,   26,   53,   97,    8,   45,   14,   46,  436,   12,  672,   19,\n",
      "        4626,    9, 3563, 1270,   10,    4,  320,   15,    9, 3022,   14,    4,\n",
      "        1437,  107, 2437, 1038,   19,   46,  177,    6,    3], device='cuda:0')\n",
      "Epoch 39 => Train Loss 3.4238817064385665 Train PPL 30.68830711898637\n",
      "Epoch 39 => Vaid Loss 3.5165009161189227 Valid PPL 33.66642049507437\n",
      "tensor([  7,  13,  36,  11,   4,  32,  11,  38,  10,   4,   0, 575,   6,   3,\n",
      "          6,   0,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6, 165,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6], device='cuda:0') tensor([   7,   78,   36,    8,    4, 3441,   11,   33,    8,    4,  315,  248,\n",
      "           6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 40 => Train Loss 3.4029445480882075 Train PPL 30.05246080831794\n",
      "Epoch 40 => Vaid Loss 3.5063479935394 Valid PPL 33.326337274922516\n",
      "tensor([  7, 110,   8,   4,  31,  14,   4,   6,   4,   6, 162,   6,   6,   6,\n",
      "          6,   6,   6,   6, 190,   6,   6,   6,   6, 165, 190,   6,   6, 165,\n",
      "          6,   6,   6,   6,   6,   6,   6, 190,   6,   6], device='cuda:0') tensor([4015,  110,  958,    4,  539,   14, 3088,   19, 2032,   28, 2461,    3,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "Epoch 41 => Train Loss 3.3894033390179015 Train PPL 29.648257033705935\n",
      "Epoch 41 => Vaid Loss 3.50645900289923 Valid PPL 33.33003701563802\n",
      "tensor([ 7,  8,  4, 37,  6,  6,  4,  8,  4,  0,  6,  3,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([ 112,  145,  223, 2027,  777,   30,   38,   85,    4,  450,    6,    3,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 42 => Train Loss 3.3863058717627275 Train PPL 29.55656460882326\n",
      "Epoch 42 => Vaid Loss 3.5064767841286444 Valid PPL 33.33062966994162\n",
      "tensor([  7,  13, 110,   8,  39,   6,   3,  11,   6,   6,   3,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6], device='cuda:0') tensor([   7,  203,  110,   11, 2447,    6,    3,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 43 => Train Loss 3.3797033836967065 Train PPL 29.362060555647503\n",
      "Epoch 43 => Vaid Loss 3.4995693547956104 Valid PPL 33.10119401839226\n",
      "tensor([  7,  23,  18,  56,  67,  14,   4,   0,   6,   3,   6,   3,   6, 165,\n",
      "          6,   0,   6,   6,   6,   6,   6,   6,   6,   6,   3,   6,   6,   6,\n",
      "        165,   6,   6,   6, 165,   6,   6,   6,   6,   6,   6,   6,   0],\n",
      "       device='cuda:0') tensor([ 146,  267,  415,  425, 2753,   14,    4,  362,    6,    3,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 44 => Train Loss 3.376394451710216 Train PPL 29.26506406019029\n",
      "Epoch 44 => Vaid Loss 3.502260428443698 Valid PPL 33.19039173431689\n",
      "tensor([ 7, 13,  8,  4, 31,  6,  4,  6,  6, 33, 42,  6,  4,  0,  6,  6,  3,  6,\n",
      "         6,  6,  6,  0,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
      "       device='cuda:0') tensor([   7,   17,  156,    4,  633,   10,   46,  177,   11,   43, 1378,   14,\n",
      "           4,   59,   58,    6,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 45 => Train Loss 3.3575612202025296 Train PPL 28.71906592655635\n",
      "Epoch 45 => Vaid Loss 3.489546912661671 Valid PPL 32.77109617322593\n",
      "tensor([  7,  18,  33,   8,   4,   4,  42,   4,  31,  14,   4,  11,   4,  23,\n",
      "          6,   3,   6,   6,   6,   4,   6, 190,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6,   6,   6, 190], device='cuda:0') tensor([  84,   18,  364,   88,   12,   43, 1430,    4,  623,   14,  220,   12,\n",
      "          76,  598,    6,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n",
      "Epoch 46 => Train Loss 3.345298930218345 Train PPL 28.3690547669965\n",
      "Epoch 46 => Vaid Loss 3.4912565412841134 Valid PPL 32.82717049670872\n",
      "tensor([ 7, 13, 11,  4, 32, 30,  4,  0,  6,  3, 11,  6,  4,  6,  6,  0,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6], device='cuda:0') tensor([   7,   13,  218,    4,  567,    8,    4, 1929,    6,    3,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 47 => Train Loss 3.336837308448658 Train PPL 28.130019295692698\n",
      "Epoch 47 => Vaid Loss 3.4878469868994793 Valid PPL 32.71543506586485\n",
      "tensor([  7,  13,   8,   4,  31,   8,  10,   4,   0, 115,   6,  12,   4, 484,\n",
      "          6,   4,   6,   3,   6,   6, 165,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6,   6,   6], device='cuda:0') tensor([  7,  17,  12,   4,  13, 155,  10,   4, 340, 149,  16,  15, 423, 484,\n",
      "         99, 160,   6,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1], device='cuda:0')\n",
      "Epoch 48 => Train Loss 3.328035651591786 Train PPL 27.883514933252634\n",
      "Epoch 48 => Vaid Loss 3.5049280769961357 Valid PPL 33.279050237067985\n",
      "tensor([  7,  13,   8,  38,   4,   0, 119,   6,   4,   0, 564,  35,   6,   3,\n",
      "          6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "          6,   6], device='cuda:0') tensor([   7,   34,   11,   39,    4,  804,  138,   10,    4,  749, 1319,  684,\n",
      "           6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1], device='cuda:0')\n",
      "Epoch 49 => Train Loss 3.324587821960449 Train PPL 27.78754286697942\n",
      "Epoch 49 => Vaid Loss 3.5095483780612606 Valid PPL 33.43316522298148\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model = TransformerWrapper(emb_size=512, src_vocab_size=len(de_vocab), tgt_vocab_size=len(en_vocab), device=device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "EPOCH = 50\n",
    "GCLIP = 1\n",
    "for e in range(EPOCH):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        # src = src[:, 1:7]\n",
    "        # tgt = tgt[:, :7]\n",
    "\n",
    "        # src[:, -3:] = PAD_IDX\n",
    "        # tgt[:, -2:] = PAD_IDX \n",
    "\n",
    "        tgt_in = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = generate_mask(src, tgt_in)\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = src_mask.to(device), tgt_mask.to(device), src_padding_mask.to(device), tgt_padding_mask.to(device)\n",
    "        \n",
    "\n",
    "        logits = model(src, tgt_in, src_mask=src_mask, tgt_mask=tgt_mask, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, memory_padding_mask=src_padding_mask)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # print(logits.reshape(-1, logits.size(-1)).shape)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        # print(tgt_out, tgt_out.shape)\n",
    "        # print(tgt_out.reshape(-1))\n",
    "        epoch_loss += loss.item()\n",
    "        # break\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GCLIP)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "    print(preds[-1], tgt_out[-1])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for src, tgt in valid_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_in = tgt[:, :-1]\n",
    "            tgt_out = tgt[:, 1:]\n",
    "            \n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = generate_mask(src, tgt_in)\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = src_mask.to(device), tgt_mask.to(device), src_padding_mask.to(device), tgt_padding_mask.to(device)\n",
    "\n",
    "            logits = model(src, tgt_in, src_mask=src_mask, tgt_mask=tgt_mask, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, memory_padding_mask=src_padding_mask)\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {e} => Train Loss {epoch_loss / len(train_loader)} Train PPL {math.exp(epoch_loss / len(train_loader))}\")\n",
    "    print(f\"Epoch {e} => Vaid Loss {valid_loss / len(valid_loader)} Valid PPL {math.exp(valid_loss / len(valid_loader))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "# from torch import optim\n",
    "\n",
    "# class TransformerTrainer(pl.LightningModule):\n",
    "#     def __init__(self, model) -> None:\n",
    "#         super().__init__()\n",
    "#         self.transformer = model\n",
    "#         self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         src, tgt = batch    # (N, L)\n",
    "#         tgt_input = tgt[:, :-1]\n",
    "\n",
    "#         src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.generate_mask(src, tgt_input)\n",
    "#         src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = src_mask.to(self.device), tgt_mask.to(self.device), src_padding_mask.to(self.device), tgt_padding_mask.to(self.device)\n",
    "\n",
    "#         logits = self.transformer(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, memory_padding_mask=src_padding_mask)    # (N, L, tgt_vacab_size)\n",
    "#         loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "#         self.log('train_loss', loss)\n",
    "#         self.log('train_ppl', math.exp(loss))\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         src, tgt = batch\n",
    "#         tgt_input = tgt[:, :-1]\n",
    "\n",
    "#         _, tgt_mask, _, _ = self.generate_mask(src, tgt_input)\n",
    "#         tgt_mask = tgt_mask.to(self.device)\n",
    "\n",
    "#         logits = self.transformer(src, tgt_input, tgt_mask=tgt_mask)    # (N, L, tgt_vacab_size)\n",
    "#         loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "#         self.log('val_loss', loss)\n",
    "#         self.log('val_ppl', math.exp(loss))\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.Adam(self.parameters(), lr=5e-4)\n",
    "#         return optimizer\n",
    "    \n",
    "#     def generate_mask(self, src: Tensor, tgt: Tensor) -> Tuple[Tensor]:\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             src: [N, L]\n",
    "#             tgt: [N, L]\n",
    "#         \"\"\"\n",
    "#         src_seq_len = src.shape[-1] # L\n",
    "#         tgt_seq_len = tgt.shape[-1]\n",
    "\n",
    "#         \"\"\"\n",
    "#         tgt mask\n",
    "#         1 0 0 0 0 \n",
    "#         1 1 0 0 0\n",
    "#         1 1 1 0 0\n",
    "#         1 1 1 1 0\n",
    "#         1 1 1 1 1\n",
    "#         \"\"\"\n",
    "#         tgt_mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1).float()\n",
    "#         src_mask = torch.ones((src_seq_len, src_seq_len)).type(torch.bool)\n",
    "#         src_padding_mask = (src != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "#         tgt_padding_mask = (tgt != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "#         return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# # early_stop_callback = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
    "# if torch.cuda.is_available():\n",
    "#     trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=20, gradient_clip_val=1)#, callbacks=[early_stop_callback])\n",
    "# else:\n",
    "#     trainer = pl.Trainer(max_epochs=20, gradient_clip_val=1)\n",
    "\n",
    "# model = TransformerWrapper(emb_size=512, src_vocab_size=len(en_vocab), tgt_vocab_size=len(de_vocab))\n",
    "# pl_model = TransformerTrainer(model=model)\n",
    "# trainer.fit(model=pl_model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d54d44b476ee03c59f2b2873b2d13a0a58dbc0ce917a1197d2792a3bd70a14c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
