{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer and build vocabulary\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_md')\n",
    "\n",
    "def yield_tokens(data_iter, language: str):\n",
    "    for text in data_iter:\n",
    "        if language == 'en':\n",
    "            yield en_tokenizer(text[0])\n",
    "        elif language == 'de':\n",
    "            yield de_tokenizer(text[1])\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'en'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "de_vocab = build_vocab_from_iterator(yield_tokens(Multi30k(split='train', language_pair=('en', 'de')), 'de'), specials=special_tokens, special_first=True, min_freq=3)\n",
    "en_vocab.set_default_index(UNK_IDX) # oov 일때 반환하는 토큰\n",
    "de_vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166, 37, 8, 336, 288, 18, 1225, 4, 759, 4497, 2958, 6]\n",
      "[85, 32, 11, 848, 2209, 16, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "# Set preprocess pipeline\n",
    "en_pipeline = lambda x: en_vocab(en_tokenizer(x))\n",
    "de_pipeline = lambda x: de_vocab(de_tokenizer(x))\n",
    "trasnform_pipeline = {'en': en_pipeline, 'de': de_pipeline}\n",
    "print(en_pipeline('Several men in hard hats are operating a giant pulley system.'))\n",
    "print(de_pipeline('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 363/15514 [00:29<20:50, 12.12it/s, loss=5.65, v_num=0]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_func(batch, src_ln: str = 'en', tgt_ln: str = 'de', batch_first: bool = True):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src, tgt in batch:\n",
    "        src_ids = trasnform_pipeline[src_ln](src.rstrip('\\n'))\n",
    "        tgt_ids = trasnform_pipeline[tgt_ln](tgt.rstrip('\\n'))\n",
    "        src_ids = torch.cat((torch.tensor([BOS_IDX]), torch.tensor(src_ids), torch.tensor([EOS_IDX])))\n",
    "        tgt_ids = torch.cat((torch.tensor([BOS_IDX]), torch.tensor(tgt_ids), torch.tensor([EOS_IDX])))\n",
    "        src_batch.append(src_ids)\n",
    "        tgt_batch.append(tgt_ids)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=batch_first)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=batch_first)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), language_pair=('en', 'de'))\n",
    "train_loader = DataLoader(train_iter, batch_size=64, collate_fn=collate_func)\n",
    "valid_loader = DataLoader(valid_iter, batch_size=1, collate_fn=collate_func)\n",
    "test_loader = DataLoader(test_iter, batch_size=1, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer\n",
    "# I follow style of official Pytorch Transformer source code and adjust it simply\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.modules import LayerNorm\n",
    "from typing import Union, Callable, Optional, Any, Tuple\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_head: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, \\\n",
    "        dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \\\n",
    "        layer_norm_eps: float = 1e-5, batch_first: bool = True, norm_first: bool = False, device=None, dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Transformer, self).__init__()\n",
    "   \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, n_head, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)\n",
    "        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_head, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)\n",
    "        decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_params()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, \\\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output  = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "    \n",
    "    def _reset_params(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dim_feedforward: int = 2048, dropout: float = 0.1, \\\n",
    "                activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,  \n",
    "                device=None, dtype=None) -> None:\n",
    "\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Define params\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, dropout, batch_first, **factory_kwargs)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.droput = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._self_attn_block(self.norm1(src), src_mask, src_key_padding_mask)\n",
    "            x = x + self._feedforward_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._self_attn_block(x, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._feedforward_block(x))\n",
    "        return x\n",
    "\n",
    "    def _self_attn_block(self, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    def _feedforward_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.droput(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dim_feedforward: int = 2048, droput: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu, \\\n",
    "                layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        # Define params\n",
    "        self.self_attn = MultiheadAttention(d_model, n_head, droput, batch_first, **factory_kwargs)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, n_head, droput, batch_first, **factory_kwargs)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.droput = nn.Dropout(droput)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "        \n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(droput)\n",
    "        self.dropout2 = nn.Dropout(droput)\n",
    "        self.dropout3 = nn.Dropout(droput)\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, \\\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._self_attn_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._multihead_attn_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
    "            x = x + self._feedforward_block(self.norm3(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._self_attn_block(x, tgt_mask ,tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._multihead_attn_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "            x = self.norm3(x + self._feedforward_block(x))\n",
    "        return x\n",
    "    \n",
    "    def _self_attn_block(self, x: Tensor, attn_mask: Optional[Tensor] = None, key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = self.self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _multihead_attn_block(self, x: Tensor, mem: Tensor, attn_mask: Optional[Tensor] = None, key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        x = self.multihead_attn(x, mem, mem, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        return self.dropout2(x)\n",
    "    \n",
    "    def _feedforward_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.droput(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0, batch_first=False, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = int(embed_dim // num_heads)\n",
    "        assert self.embed_dim == self.head_dim * num_heads\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **factory_kwargs)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, need_weights: bool = True, \\\n",
    "                attn_mask: Optional[Tensor] = None, average_attn_weights: bool = True) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        query, key, value = self.wq(query), self.wk(key), self.wv(value)\n",
    "        query, key ,value = self._split_heads(query), self._split_heads(key), self._split_heads(value)\n",
    "        attn_out = self._attention(query, key, value, key_padding_mask, attn_mask)\n",
    "        attn_out = self.out_proj(attn_out)\n",
    "        return attn_out\n",
    "    \n",
    "    def _split_heads(self, proj: Tensor) -> Tensor:\n",
    "        if self.batch_first:    # (N, L, E)\n",
    "            bs = proj.size(0)\n",
    "            proj = proj.view(bs, -1, self.num_heads, self.head_dim)    # (N, L, H, E_Hi)\n",
    "            proj = proj.transpose(1, 2) # (N, H, L, E_Hi)\n",
    "        else:   # (L, N, E)\n",
    "            bs = proj.size(1)\n",
    "            proj = proj.view(-1, bs, self.num_heads, self.head_dim)    # (L, N, H, E_Hi)\n",
    "        return proj\n",
    "    \n",
    "    def _attention(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None, attn_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        if self.batch_first:\n",
    "            score = torch.matmul(query, key.transpose(-1, -2))  # (N, H, QL, KL), score = Q * K^T\n",
    "            score = score / math.sqrt(query.size(-1)) # score = Q * K^T divided by sqrt(E_Hi)\n",
    "            if key_padding_mask is not None:\n",
    "                score = score.masked_fill(key_padding_mask == 0, float('-inf'))\n",
    "            if attn_mask is not None:\n",
    "                score = score.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "            softmax = F.softmax(score, dim=-1)\n",
    "            attn_out = torch.matmul(softmax, value)   # (N, H, L, E_Hi)\n",
    "            attn_out = attn_out.transpose(1, 2) # (N, L, H, E_Hi)\n",
    "            attn_out = attn_out.contiguous().view(attn_out.size(0), -1, self.embed_dim)  # (N, L, E)\n",
    "            return attn_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int = 512, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1)\n",
    "        val = torch.exp(-torch.arange(0, emb_size, 2) / emb_size * math.log(10000))\n",
    "        pos_encoding = torch.zeros((max_len, emb_size))\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos * val)\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos * val)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)    # batch first\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "    \n",
    "    def forward(self, token_embedding: Tensor) -> Tensor:\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:, :token_embedding.size(1), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int = 512) -> None:\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def forward(self, tokens: Tensor) -> Tensor:\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWrapper(nn.Module):\n",
    "    def __init__(self, emb_size: int, src_vocab_size: int, tgt_vocab_size: int, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size, batch_first=True)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, src_padding_mask: Optional[Tensor] = None, tgt_padding_mask: Optional[Tensor] = None, memory_padding_mask: Optional[Tensor] = None):\n",
    "        src_emb = self.pos_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.pos_encoding(self.tgt_tok_emb(tgt))\n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask, src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_padding_mask)\n",
    "        return self.generator(output)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        src_emb = self.pos_encoding(self.src_tok_emb(src))\n",
    "        return self.transformer.encoder(src_emb, src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor) -> Tensor:\n",
    "        tgt_emb = self.pos_encoding(self.tgt_tok_emb(tgt))\n",
    "        return self.transformer.decoder(tgt_emb, memory, tgt_mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "\n",
    "class TransformerTrainer(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.transformer = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, tgt = batch    # (N, L)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.generate_mask(src, tgt_input)\n",
    "\n",
    "        logits = self.transformer(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, memory_padding_mask=src_padding_mask)    # (N, L, tgt_vacab_size)\n",
    "        loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        self.log('train loss', loss)\n",
    "        self.log('train ppl', math.exp(loss))\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, tgt = batch\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.generate_mask(src, tgt_input)\n",
    "\n",
    "        logits = self.transformer(src, tgt_input, tgt_mask=tgt_mask)    # (N, L, tgt_vacab_size)\n",
    "        loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        self.log('valid loss', loss)\n",
    "        self.log('valid ppl', math.exp(loss))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def generate_mask(src: Tensor, tgt: Tensor) -> Tuple[Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [N, L]\n",
    "            tgt: [N, L]\n",
    "        \"\"\"\n",
    "        src_seq_len = src.shape[-1] # L\n",
    "        tgt_seq_len = tgt.shape[-1]\n",
    "\n",
    "        \"\"\"\n",
    "        tgt mask\n",
    "        1 0 0 0 0 \n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 1 0\n",
    "        1 1 1 1 1\n",
    "        \"\"\"\n",
    "        tgt_mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1).float()\n",
    "        src_mask = torch.ones((src_seq_len, src_seq_len)).type(torch.bool)\n",
    "        src_padding_mask = (src != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "        tgt_padding_mask = (tgt != PAD_IDX).float().unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | transformer | TransformerWrapper | 52.2 M\n",
      "1 | loss_fn     | CrossEntropyLoss   | 0     \n",
      "---------------------------------------------------\n",
      "52.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "52.2 M    Total params\n",
      "208.639   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun_jung/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:142: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  \"Your `IterableDataset` has `__len__` defined.\"\n",
      "/home/wonjun_jung/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/wonjun_jung/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/wonjun_jung/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/utilities/data.py:93: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  \"Your `IterableDataset` has `__len__` defined.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1468 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Early stopping conditioned on metric `valid loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: ``",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a3aa6b92f901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpl_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpl_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         )\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_run_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36mon_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# call train epoch end hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0mcallback_fx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_fx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m                 \u001b[0mcallback_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0;31m# next call hook in lightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;34m\"\"\"Called when the epoch ends.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_validation_epoch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/callbacks/early_stopping.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_on_train_epoch_end\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_skip_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_early_stopping_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_validation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.LightningModule\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/callbacks/early_stopping.py\u001b[0m in \u001b[0;36m_run_early_stopping_check\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         ):  # short circuit if metric not present\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-Paper-Reproduce/.venv/lib/python3.6/site-packages/pytorch_lightning/callbacks/early_stopping.py\u001b[0m in \u001b[0;36m_validate_condition_metric\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmonitor_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mrank_zero_warn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `valid loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: ``"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='valid loss', mode='min', patience=3)\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=20, profiler=\"simple\", callbacks=[early_stop_callback])\n",
    "model = TransformerWrapper(emb_size=512, src_vocab_size=len(en_vocab), tgt_vocab_size=len(de_vocab))\n",
    "pl_model = TransformerTrainer(model=model)\n",
    "trainer.fit(model=pl_model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d54d44b476ee03c59f2b2873b2d13a0a58dbc0ce917a1197d2792a3bd70a14c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
